<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CentOS7部署DVWA环境]]></title>
    <url>%2F2018%2F07%2F30%2FDVWA%2F2018-07-30-CentOS7%E9%83%A8%E7%BD%B2DVWA%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[DVWA下载DVWA 的官方网站：http://www.dvwa.co.uk/DVWA 的Github页面：https://github.com/ethicalhack3r/DVWA CentOS7使用命令 wget https://github.com/ethicalhack3r/DVWA/archive/master.zip 来下载 DVWA。 如有提示bash: apt: command not found…的，请使用 yum install wget 命令安装 wget 工具。 使用 unzip DVWA-master.zip 来获得 DVWA 文件夹。 安装DVWA解压文件将之前解压出的 DVWA-master 文件夹放入 /var/www/html/ 修改DVWA-master文件权限chmod -R 777 /var/www/html/DVWA-master 启动相关程序服务systemctl restart httpdsystemctl restart mariadb 创建数据库mysql -u root -pCREATE DATABASE dvwa; 修改配置信息此时访问 http://1127.0.0.1/DVWA-master 会出现如下错误提示 DVWA System error - config file not found. Copy config/config.inc.php.dist to config/config.inc.php and configure to your environment. 根据提示，进入 /var/www/html/DVWA-master/config，使用 cp config.inc.php.dist config.inc.php 将配置文件复制一份，然后输入 vim config.inc.php 开始编辑配置文件 &lt;?php# If you are having problems connecting to the MySQL database and all of the variables below are correct# try changing the 'db_server' variable from localhost to 127.0.0.1. Fixes a problem due to sockets.# Thanks to @digininja for the fix.# Database management system to use$DBMS = 'MySQL';#$DBMS = 'PGSQL'; // Currently disabled# Database variables# WARNING: The database specified under db_database WILL BE ENTIRELY DELETED during setup.# Please use a database dedicated to DVWA.## If you are using MariaDB then you cannot use root, you must use create a dedicated DVWA user.# See README.md for more information on this.$_DVWA = array();$_DVWA[ 'db_server' ] = '127.0.0.1';$_DVWA[ 'db_database' ] = 'dvwa';$_DVWA[ 'db_user' ] = 'root';$_DVWA[ 'db_password' ] = '123456';# Only used with PostgreSQL/PGSQL database selection.$_DVWA[ 'db_port '] = '3306';# ReCAPTCHA settings# Used for the 'Insecure CAPTCHA' module# You'll need to generate your own keys at: https://www.google.com/recaptcha/admin/create$_DVWA[ 'recaptcha_public_key' ] = '6LcqImcUAAAAAJeEivmYoOMie7Vhpk6zejCglOyB';$_DVWA[ 'recaptcha_private_key' ] = '6LcqImcUAAAAAOLwu0p0REDiPb1RQqxMSTZ9OQB0';# Default security level# Default value for the secuirty level with each session.# The default is 'impossible'. You may wish to set this to either 'low', 'medium', 'high' or impossible'.$_DVWA[ 'default_security_level' ] = 'impossible';# Default PHPIDS status# PHPIDS status with each session.# The default is 'disabled'. You can set this to be either 'enabled' or 'disabled'.$_DVWA[ 'default_phpids_level' ] = 'disabled';# Verbose PHPIDS messages# Enabling this will show why the WAF blocked the request on the blocked request.# The default is 'disabled'. You can set this to be either 'true' or 'false'.$_DVWA[ 'default_phpids_verbose' ] = 'false';?&gt; 错误问题处理 问题：PHP function allow_url_include: Disabled处理：vim /etc/php.iniallow_url_include = On 问题：PHP module gd: Missing处理：yum install -y php-gd 问题：reCAPTCHA key: Missing 处理：编缉dvwa/config/config.inc.phpvim /var/www/thml/DVWA-master/config/config.inc.php$_DVWA[ 'recaptcha_public_key' ] = '6LcqImcUAAAAAJeEivmYoOMie7Vhpk6zejCglOyB';$_DVWA[ 'recaptcha_private_key' ] = '6LcqImcUAAAAAOLwu0p0REDiPb1RQqxMSTZ9OQB0'; 问题：Unable to connect to the database处理： vim /var/www/thml/DVWA-master/config/config.inc.php$_DVWA[ 'db_password' ] = '123456'; 重启http 服务 点击上图中的Create/Reset Database后会创建数据库，完后自动跳转登录页面。 登陆地址：http://172.16.32.100/DVWA-master/login.php]]></content>
      <categories>
        <category>DVWA</category>
      </categories>
      <tags>
        <tag>DVWA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(八)-Cinder服务部署]]></title>
    <url>%2F2018%2F06%2F26%2FOpenStack%2F2018-06-26-openstack_queens%E5%AE%9E%E6%88%98(%E5%85%AB)%2F</url>
    <content type="text"><![CDATA[Cinder服务介绍安装Cinder服务安装支持的软件包#安装LVMyum install lvm2 device-mapper-persistent-data#设置LVM服务开机启动systemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.service[root@cinder01 ~]# systemctl enable lvm2-lvmetad.service &amp;&amp; systemctl start lvm2-lvmetad.service[root@cinder01 ~]# systemctl status lvm2-lvmetad.service● lvm2-lvmetad.service - LVM2 metadata daemon Loaded: loaded (/usr/lib/systemd/system/lvm2-lvmetad.service; static; vendor preset: enabled) Active: active (running) since Tue 2018-09-25 09:41:49 EDT; 2min 1s ago Docs: man:lvmetad(8) Main PID: 527 (lvmetad) CGroup: /system.slice/lvm2-lvmetad.service └─527 /usr/sbin/lvmetad -fSep 25 09:41:49 cinder01 systemd[1]: Started LVM2 metadata daemon.Sep 25 09:41:49 cinder01 systemd[1]: Starting LVM2 metadata daemon... 创建LVM物理逻辑卷/dev/sdb1[root@cinder01 ~]# pvcreate /dev/sdb1 WARNING: Device for PV 4MsZHW-iM6g-QGtf-ty1L-760q-1Qoo-oI2Vhr not found or rejected by a filter. WARNING: Device for PV 4MsZHW-iM6g-QGtf-ty1L-760q-1Qoo-oI2Vhr not found or rejected by a filter. WARNING: Device for PV 4MsZHW-iM6g-QGtf-ty1L-760q-1Qoo-oI2Vhr not found or rejected by a filter. WARNING: Couldn't find all devices for LV centos/swap while checking used and assumed devices. WARNING: Couldn't find all devices for LV centos/home while checking used and assumed devices. WARNING: Couldn't find all devices for LV centos/root while checking used and assumed devices.WARNING: ext4 signature detected on /dev/sdb1 at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/sdb1. Physical volume "/dev/sdb1" successfully created. 创建cinder-volumes逻辑卷组[root@cinder01 ~]# vgcreate cinder-volumes /dev/sdb1 Volume group "cinder-volumes" successfully created[root@cinder01 ~]# vgcreate cinder-volumes /dev/sdb1 WARNING: Device for PV 4MsZHW-iM6g-QGtf-ty1L-760q-1Qoo-oI2Vhr not found or rejected by a filter. WARNING: Device for PV 4MsZHW-iM6g-QGtf-ty1L-760q-1Qoo-oI2Vhr not found or rejected by a filter. WARNING: Device for PV 4MsZHW-iM6g-QGtf-ty1L-760q-1Qoo-oI2Vhr not found or rejected by a filter. WARNING: Couldn't find all devices for LV centos/swap while checking used and assumed devices. WARNING: Couldn't find all devices for LV centos/home while checking used and assumed devices. WARNING: Couldn't find all devices for LV centos/root while checking used and assumed devices. Volume group "cinder-volumes" successfully created 编辑/etc/lvm/lvm.conf文件并#在devices部分中，添加一个接受/ dev / sdb设备的过滤器并拒绝所有其他设备：# vim /etc/lvm/lvm.confdevices &#123;...filter = [ "a/sdb1/", "r/.*/"]&#125; 安装和配置组件安装软件包yum install openstack-cinder targetcli python-keystone -y 编辑/etc/cinder/cinder.conf[DEFAULT]transport_url = rabbit://openstack:123456@controllerauth_strategy = keystonemy_ip = 10.10.10.40enabled_backends = lvmglance_api_servers = http://controller:9292[database]connection = mysql+pymysql://cinder:123456@controller/cinder[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = cinderpassword = 123456#在[lvm]部分中，使用LVM驱动程序，cinder-volumes卷组，iSCSI协议和相应的iSCSI服务配置LVM后端。 如果[lvm]部分不存在，请创建它：[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadm[oslo_concurrency]lock_path = /var/lib/cinder/tmp 设置存储服务开机启动systemctl enable openstack-cinder-volume.service target.servicesystemctl start openstack-cinder-volume.service target.service [root@cinder01 ~]# systemctl start openstack-cinder-volume.service target.service[root@cinder01 ~]# systemctl status openstack-cinder-volume.service target.service● openstack-cinder-volume.service - OpenStack Cinder Volume Server Loaded: loaded (/usr/lib/systemd/system/openstack-cinder-volume.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2018-09-25 10:05:56 EDT; 696ms ago Main PID: 31629 (cinder-volume) CGroup: /system.slice/openstack-cinder-volume.service └─31629 /usr/bin/python2 /usr/bin/cinder-volume --config-file /usr/share/cinder/cinder-dist.conf -...Sep 25 10:05:56 cinder01 systemd[1]: Started OpenStack Cinder Volume Server.Sep 25 10:05:56 cinder01 systemd[1]: Starting OpenStack Cinder Volume Server...● target.service - Restore LIO kernel target configuration Loaded: loaded (/usr/lib/systemd/system/target.service; enabled; vendor preset: disabled) Active: active (exited) since Tue 2018-09-25 10:05:48 EDT; 9s ago Process: 31414 ExecStart=/usr/bin/targetctl restore (code=exited, status=0/SUCCESS) Main PID: 31414 (code=exited, status=0/SUCCESS) CGroup: /system.slice/target.serviceSep 25 10:05:47 cinder01 systemd[1]: Starting Restore LIO kernel target configuration...Sep 25 10:05:48 cinder01 target[31414]: No saved config file at /etc/target/saveconfig.json, ok, exitingSep 25 10:05:48 cinder01 systemd[1]: Started Restore LIO kernel target configuration.]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack</tag>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(七)-Neutron服务部署]]></title>
    <url>%2F2018%2F06%2F25%2FOpenStack%2F2018-06-25-openstack_queens%E5%AE%9E%E6%88%98(%E4%B8%83)%2F</url>
    <content type="text"><![CDATA[Neutorn介绍安装和配置controller节点配置Neutron数据库创建nuetron数据库和授权mysql -u root -pCREATE DATABASE neutron;GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY '123456'; 创建服务[root@controller01 ~]# source admin-openrc.sh [root@controller01 ~]# openstack user create --domain default --password-prompt neutronUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | cac29271f9cb4faaa652cb700bf118b8 || name | neutron || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+ 添加admin角色为neutron用户[root@controller01 ~]# openstack role add --project service --user neutron admin 创建neutron服务[root@controller01 ~]# openstack service create --name neutron --description "OpenStack Networking" network+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | OpenStack Networking || enabled | True || id | 0c71faee59ad4a98a244342be53b4eeb || name | neutron || type | network |+-------------+----------------------------------+ 创建网络服务端点[root@controller01 ~]# openstack endpoint create --region RegionOne network public http://controller:9696+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | b91e8987c82b40fdbb3ef883f54cf168 || interface | public || region | RegionOne || region_id | RegionOne || service_id | 0c71faee59ad4a98a244342be53b4eeb || service_name | neutron || service_type | network || url | http://controller:9696 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne network internal http://controller:9696+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 7922b103dbd54c4f961d36c46c99fe17 || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 0c71faee59ad4a98a244342be53b4eeb || service_name | neutron || service_type | network || url | http://controller:9696 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne network admin http://controller:9696+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | aba9ac75ef5947e3aceec18fba6b9fb8 || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 0c71faee59ad4a98a244342be53b4eeb || service_name | neutron || service_type | network || url | http://controller:9696 |+--------------+----------------------------------+ 配置网络选项安装组件yum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables ipset 配置服务组件#编辑 /etc/neutron/neutron.conf[database]connection = mysql+pymysql://neutron:123456@controller/neutron[DEFAULT]auth_strategy = keystonecore_plugin = ml2service_plugins = routertransport_url = rabbit://openstack:openstack@controllernotify_nova_on_port_status_changes = truenotify_nova_on_port_data_changes = true[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 配置网络二层插件#编辑/etc/neutron/plugins/ml2/ml2_conf.ini[ml2]type_drivers = flat,vlan,vxlantenant_network_types = vxlanmechanism_drivers = linuxbridge,l2populationextension_drivers = port_security[ml2_type_flat]flat_networks = provider[securitygroup]enable_ipset = true[ml2_type_vxlan]vni_ranges = 1:1000 配置Linux网桥#编辑 /etc/neutron/plugins/ml2/linuxbridge_agent.ini[linux_bridge]physical_interface_mappings = provider:ens33[vxlan]enable_vxlan = truelocal_ip = 20.20.20.10l2_population = true[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 配置三层代理Layer-3 (L3) agent 为自定义虚拟网络提供路由和 NAT 服务。#编辑/etc/neutron/l3_agent.ini 文件并完成下列操作：#在[DEFAULT]小节，配置 Linux bridge 接口驱动和外部网络网桥：[DEFAULT]...interface_driver = linuxbridge 配置DHCP服务#编辑 /etc/neutron/dhcp_agent.ini[DEFAULT]interface_driver = linuxbridgedhcp_driver = neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata = true 配置metadata#编辑 /etc/neutron/metadata_agent.ini[DEFAULT]nova_metadata_host = controllermetadata_proxy_shared_secret = 123456 配置计算服务使用 neutron 网络#编辑/etc/nova/nova.conf文件[neutron]url = http://controller:9696auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456service_metadata_proxy = truemetadata_proxy_shared_secret = 123456 完成安装网络服务初始化脚本/etc/neutron/plugin.ini 实际上是一个链接文件，它指向 ML2 插件的配置文件/etc/neutron/plugins/ml2/ml2_conf.ini，如果该链接文件不存在，则需要使用下面的命令创建： [root@controller01 ~]# ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini[root@controller01 ~]# ls -lh /etc/neutron/plugin.inilrwxrwxrwx. 1 root root 37 Sep 24 08:37 /etc/neutron/plugin.ini -&gt; /etc/neutron/plugins/ml2/ml2_conf.ini 初始化数据库[root@controller01 ~]# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutronINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL. Running upgrade for neutron ...INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; kilo, kilo_initialINFO [alembic.runtime.migration] Running upgrade kilo -&gt; 354db87e3225, nsxv_vdr_metadata.pyINFO [alembic.runtime.migration] Running upgrade 354db87e3225 -&gt; 599c6a226151, neutrodb_ipamINFO [alembic.runtime.migration] Running upgrade 599c6a226151 -&gt; 52c5312f6baf, Initial operations in support of address scopesINFO [alembic.runtime.migration] Running upgrade 52c5312f6baf -&gt; 313373c0ffee, Flavor frameworkINFO [alembic.runtime.migration] Running upgrade 313373c0ffee -&gt; 8675309a5c4f, network_rbacINFO [alembic.runtime.migration] Running upgrade 8675309a5c4f -&gt; 45f955889773, quota_usageINFO [alembic.runtime.migration] Running upgrade 45f955889773 -&gt; 26c371498592, subnetpool hashINFO [alembic.runtime.migration] Running upgrade 26c371498592 -&gt; 1c844d1677f7, add order to dnsnameserversINFO [alembic.runtime.migration] Running upgrade 1c844d1677f7 -&gt; 1b4c6e320f79, address scope support in subnetpoolINFO [alembic.runtime.migration] Running upgrade 1b4c6e320f79 -&gt; 48153cb5f051, qos db changesINFO [alembic.runtime.migration] Running upgrade 48153cb5f051 -&gt; 9859ac9c136, quota_reservationsINFO [alembic.runtime.migration] Running upgrade 9859ac9c136 -&gt; 34af2b5c5a59, Add dns_name to PortINFO [alembic.runtime.migration] Running upgrade 34af2b5c5a59 -&gt; 59cb5b6cf4d, Add availability zoneINFO [alembic.runtime.migration] Running upgrade 59cb5b6cf4d -&gt; 13cfb89f881a, add is_default to subnetpoolINFO [alembic.runtime.migration] Running upgrade 13cfb89f881a -&gt; 32e5974ada25, Add standard attribute tableINFO [alembic.runtime.migration] Running upgrade 32e5974ada25 -&gt; ec7fcfbf72ee, Add network availability zoneINFO [alembic.runtime.migration] Running upgrade ec7fcfbf72ee -&gt; dce3ec7a25c9, Add router availability zoneINFO [alembic.runtime.migration] Running upgrade dce3ec7a25c9 -&gt; c3a73f615e4, Add ip_version to AddressScopeINFO [alembic.runtime.migration] Running upgrade c3a73f615e4 -&gt; 659bf3d90664, Add tables and attributes to support external DNS integrationINFO [alembic.runtime.migration] Running upgrade 659bf3d90664 -&gt; 1df244e556f5, add_unique_ha_router_agent_port_bindingsINFO [alembic.runtime.migration] Running upgrade 1df244e556f5 -&gt; 19f26505c74f, Auto Allocated Topology - aka Get-Me-A-NetworkINFO [alembic.runtime.migration] Running upgrade 19f26505c74f -&gt; 15be73214821, add dynamic routing model dataINFO [alembic.runtime.migration] Running upgrade 15be73214821 -&gt; b4caf27aae4, add_bgp_dragent_model_dataINFO [alembic.runtime.migration] Running upgrade b4caf27aae4 -&gt; 15e43b934f81, rbac_qos_policyINFO [alembic.runtime.migration] Running upgrade 15e43b934f81 -&gt; 31ed664953e6, Add resource_versions row to agent tableINFO [alembic.runtime.migration] Running upgrade 31ed664953e6 -&gt; 2f9e956e7532, tag supportINFO [alembic.runtime.migration] Running upgrade 2f9e956e7532 -&gt; 3894bccad37f, add_timestamp_to_base_resourcesINFO [alembic.runtime.migration] Running upgrade 3894bccad37f -&gt; 0e66c5227a8a, Add desc to standard attr tableINFO [alembic.runtime.migration] Running upgrade 0e66c5227a8a -&gt; 45f8dd33480b, qos dscp db additionINFO [alembic.runtime.migration] Running upgrade 45f8dd33480b -&gt; 5abc0278ca73, Add support for VLAN trunkingINFO [alembic.runtime.migration] Running upgrade 5abc0278ca73 -&gt; d3435b514502, Add device_id index to PortINFO [alembic.runtime.migration] Running upgrade d3435b514502 -&gt; 30107ab6a3ee, provisioning_blocks.pyINFO [alembic.runtime.migration] Running upgrade 30107ab6a3ee -&gt; c415aab1c048, add revisions tableINFO [alembic.runtime.migration] Running upgrade c415aab1c048 -&gt; a963b38d82f4, add dns name to portdnsesINFO [alembic.runtime.migration] Running upgrade kilo -&gt; 30018084ec99, Initial no-op Liberty contract rule.INFO [alembic.runtime.migration] Running upgrade 30018084ec99 -&gt; 4ffceebfada, network_rbacINFO [alembic.runtime.migration] Running upgrade 4ffceebfada -&gt; 5498d17be016, Drop legacy OVS and LB plugin tablesINFO [alembic.runtime.migration] Running upgrade 5498d17be016 -&gt; 2a16083502f3, Metaplugin removalINFO [alembic.runtime.migration] Running upgrade 2a16083502f3 -&gt; 2e5352a0ad4d, Add missing foreign keysINFO [alembic.runtime.migration] Running upgrade 2e5352a0ad4d -&gt; 11926bcfe72d, add geneve ml2 type driverINFO [alembic.runtime.migration] Running upgrade 11926bcfe72d -&gt; 4af11ca47297, Drop cisco monolithic tablesINFO [alembic.runtime.migration] Running upgrade 4af11ca47297 -&gt; 1b294093239c, Drop embrane plugin tableINFO [alembic.runtime.migration] Running upgrade 1b294093239c -&gt; 8a6d8bdae39, standardattributes migrationINFO [alembic.runtime.migration] Running upgrade 8a6d8bdae39 -&gt; 2b4c2465d44b, DVR sheduling refactoringINFO [alembic.runtime.migration] Running upgrade 2b4c2465d44b -&gt; e3278ee65050, Drop NEC plugin tablesINFO [alembic.runtime.migration] Running upgrade e3278ee65050 -&gt; c6c112992c9, rbac_qos_policyINFO [alembic.runtime.migration] Running upgrade c6c112992c9 -&gt; 5ffceebfada, network_rbac_externalINFO [alembic.runtime.migration] Running upgrade 5ffceebfada -&gt; 4ffceebfcdc, standard_descINFO [alembic.runtime.migration] Running upgrade 4ffceebfcdc -&gt; 7bbb25278f53, device_owner_ha_replicate_intINFO [alembic.runtime.migration] Running upgrade 7bbb25278f53 -&gt; 89ab9a816d70, Rename ml2_network_segments tableINFO [alembic.runtime.migration] Running upgrade a963b38d82f4 -&gt; 3d0e74aa7d37, Add flavor_id to RouterINFO [alembic.runtime.migration] Running upgrade 3d0e74aa7d37 -&gt; 030a959ceafa, uniq_routerports0port_idINFO [alembic.runtime.migration] Running upgrade 030a959ceafa -&gt; a5648cfeeadf, Add support for Subnet Service TypesINFO [alembic.runtime.migration] Running upgrade a5648cfeeadf -&gt; 0f5bef0f87d4, add_qos_minimum_bandwidth_rulesINFO [alembic.runtime.migration] Running upgrade 0f5bef0f87d4 -&gt; 67daae611b6e, add standardattr to qos policiesINFO [alembic.runtime.migration] Running upgrade 89ab9a816d70 -&gt; c879c5e1ee90, Add segment_id to subnetINFO [alembic.runtime.migration] Running upgrade c879c5e1ee90 -&gt; 8fd3918ef6f4, Add segment_host_mapping table.INFO [alembic.runtime.migration] Running upgrade 8fd3918ef6f4 -&gt; 4bcd4df1f426, Rename ml2_dvr_port_bindingsINFO [alembic.runtime.migration] Running upgrade 4bcd4df1f426 -&gt; b67e765a3524, Remove mtu column from networks.INFO [alembic.runtime.migration] Running upgrade 67daae611b6e -&gt; 6b461a21bcfc, uniq_floatingips0floating_network_id0fixed_port_id0fixed_ip_addrINFO [alembic.runtime.migration] Running upgrade 6b461a21bcfc -&gt; 5cd92597d11d, Add ip_allocation to portINFO [alembic.runtime.migration] Running upgrade 5cd92597d11d -&gt; 929c968efe70, add_pk_version_tableINFO [alembic.runtime.migration] Running upgrade 929c968efe70 -&gt; a9c43481023c, extend_pk_with_host_and_add_status_to_ml2_port_bindingINFO [alembic.runtime.migration] Running upgrade a9c43481023c -&gt; 804a3c76314c, Add data_plane_status to PortINFO [alembic.runtime.migration] Running upgrade 804a3c76314c -&gt; 2b42d90729da, qos add direction to bw_limit_rule tableINFO [alembic.runtime.migration] Running upgrade 2b42d90729da -&gt; 62c781cb6192, add is default to qos policiesINFO [alembic.runtime.migration] Running upgrade 62c781cb6192 -&gt; c8c222d42aa9, logging apiINFO [alembic.runtime.migration] Running upgrade c8c222d42aa9 -&gt; 349b6fd605a6, Add dns_domain to portdnsesINFO [alembic.runtime.migration] Running upgrade 349b6fd605a6 -&gt; 7d32f979895f, add mtu for networksINFO [alembic.runtime.migration] Running upgrade 7d32f979895f -&gt; 594422d373ee, fip qosINFO [alembic.runtime.migration] Running upgrade b67e765a3524 -&gt; a84ccf28f06a, migrate dns name from portINFO [alembic.runtime.migration] Running upgrade a84ccf28f06a -&gt; 7d9d8eeec6ad, rename tenant to projectINFO [alembic.runtime.migration] Running upgrade 7d9d8eeec6ad -&gt; a8b517cff8ab, Add routerport bindings for L3 HAINFO [alembic.runtime.migration] Running upgrade a8b517cff8ab -&gt; 3b935b28e7a0, migrate to pluggable ipamINFO [alembic.runtime.migration] Running upgrade 3b935b28e7a0 -&gt; b12a3ef66e62, add standardattr to qos policiesINFO [alembic.runtime.migration] Running upgrade b12a3ef66e62 -&gt; 97c25b0d2353, Add Name and Description to the networksegments tableINFO [alembic.runtime.migration] Running upgrade 97c25b0d2353 -&gt; 2e0d7a8a1586, Add binding index to RouterL3AgentBindingINFO [alembic.runtime.migration] Running upgrade 2e0d7a8a1586 -&gt; 5c85685d616d, Remove availability ranges. OK 验证数据库[root@controller01 ~]# mysql -uroot -p123456 neutron -e "show tables"+-----------------------------------------+| Tables_in_neutron |+-----------------------------------------+| address_scopes || agents || alembic_version || allowedaddresspairs || arista_provisioned_nets || arista_provisioned_tenants || arista_provisioned_vms || auto_allocated_topologies || bgp_peers || bgp_speaker_dragent_bindings || bgp_speaker_network_bindings || bgp_speaker_peer_bindings || bgp_speakers || brocadenetworks || brocadeports || cisco_csr_identifier_map || cisco_hosting_devices || cisco_ml2_apic_contracts || cisco_ml2_apic_host_links || cisco_ml2_apic_names || cisco_ml2_n1kv_network_bindings || cisco_ml2_n1kv_network_profiles || cisco_ml2_n1kv_policy_profiles || cisco_ml2_n1kv_port_bindings || cisco_ml2_n1kv_profile_bindings || cisco_ml2_n1kv_vlan_allocations || cisco_ml2_n1kv_vxlan_allocations || cisco_ml2_nexus_nve || cisco_ml2_nexusport_bindings || cisco_port_mappings || cisco_router_mappings || consistencyhashes || default_security_group || dnsnameservers || dvr_host_macs || externalnetworks || extradhcpopts || firewall_policies || firewall_rules || firewalls || flavors || flavorserviceprofilebindings || floatingipdnses || floatingips || ha_router_agent_port_bindings || ha_router_networks || ha_router_vrid_allocations || healthmonitors || ikepolicies || ipallocationpools || ipallocations || ipamallocationpools || ipamallocations || ipamsubnets || ipsec_site_connections || ipsecpeercidrs || ipsecpolicies || logs || lsn || lsn_port || maclearningstates || members || meteringlabelrules || meteringlabels || ml2_brocadenetworks || ml2_brocadeports || ml2_distributed_port_bindings || ml2_flat_allocations || ml2_geneve_allocations || ml2_geneve_endpoints || ml2_gre_allocations || ml2_gre_endpoints || ml2_nexus_vxlan_allocations || ml2_nexus_vxlan_mcast_groups || ml2_port_binding_levels || ml2_port_bindings || ml2_ucsm_port_profiles || ml2_vlan_allocations || ml2_vxlan_allocations || ml2_vxlan_endpoints || multi_provider_networks || networkconnections || networkdhcpagentbindings || networkdnsdomains || networkgatewaydevicereferences || networkgatewaydevices || networkgateways || networkqueuemappings || networkrbacs || networks || networksecuritybindings || networksegments || neutron_nsx_network_mappings || neutron_nsx_port_mappings || neutron_nsx_router_mappings || neutron_nsx_security_group_mappings || nexthops || nsxv_edge_dhcp_static_bindings || nsxv_edge_vnic_bindings || nsxv_firewall_rule_bindings || nsxv_internal_edges || nsxv_internal_networks || nsxv_port_index_mappings || nsxv_port_vnic_mappings || nsxv_router_bindings || nsxv_router_ext_attributes || nsxv_rule_mappings || nsxv_security_group_section_mappings || nsxv_spoofguard_policy_network_mappings || nsxv_tz_network_bindings || nsxv_vdr_dhcp_bindings || nuage_net_partition_router_mapping || nuage_net_partitions || nuage_provider_net_bindings || nuage_subnet_l2dom_mapping || poolloadbalanceragentbindings || poolmonitorassociations || pools || poolstatisticss || portbindingports || portdataplanestatuses || portdnses || portqueuemappings || ports || portsecuritybindings || providerresourceassociations || provisioningblocks || qos_bandwidth_limit_rules || qos_dscp_marking_rules || qos_fip_policy_bindings || qos_minimum_bandwidth_rules || qos_network_policy_bindings || qos_policies || qos_policies_default || qos_port_policy_bindings || qospolicyrbacs || qosqueues || quotas || quotausages || reservations || resourcedeltas || router_extra_attributes || routerl3agentbindings || routerports || routerroutes || routerrules || routers || securitygroupportbindings || securitygrouprules || securitygroups || segmenthostmappings || serviceprofiles || sessionpersistences || standardattributes || subnet_service_types || subnetpoolprefixes || subnetpools || subnetroutes || subnets || subports || tags || trunks || tz_network_bindings || vcns_router_bindings || vips || vpnservices |+-----------------------------------------+ 重启计算服务[root@controller01 ~]# systemctl restart openstack-nova-api.service[root@controller01 ~]# systemctl status openstack-nova-api.service● openstack-nova-api.service - OpenStack Nova API Server Loaded: loaded (/usr/lib/systemd/system/openstack-nova-api.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 08:42:10 EDT; 28s ago Main PID: 61788 (nova-api) CGroup: /system.slice/openstack-nova-api.service ├─61788 /usr/bin/python2 /usr/bin/nova-api ├─62409 /usr/bin/python2 /usr/bin/nova-api ├─62410 /usr/bin/python2 /usr/bin/nova-api ├─62411 /usr/bin/python2 /usr/bin/nova-api ├─62412 /usr/bin/python2 /usr/bin/nova-api ├─62413 /usr/bin/python2 /usr/bin/nova-api ├─62414 /usr/bin/python2 /usr/bin/nova-api ├─62418 /usr/bin/python2 /usr/bin/nova-api └─62419 /usr/bin/python2 /usr/bin/nova-apiSep 24 08:41:54 controller01 systemd[1]: Starting OpenStack Nova API Server...Sep 24 08:42:10 controller01 systemd[1]: Started OpenStack Nova API Server. 启动网络服务[root@controller01 ~]# systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/neutron-server.service to /usr/lib/systemd/system/neutron-server.service.Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-linuxbridge-agent.service to /usr/lib/systemd/system/neutron-linuxbridge-agent.service.Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-dhcp-agent.service to /usr/lib/systemd/system/neutron-dhcp-agent.service.Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-metadata-agent.service to /usr/lib/systemd/system/neutron-metadata-agent.service.Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-l3-agent.service to /usr/lib/systemd/system/neutron-l3-agent.service.[root@controller01 ~]# systemctl start neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service[root@controller01 ~]# systemctl status neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service● neutron-server.service - OpenStack Neutron Server Loaded: loaded (/usr/lib/systemd/system/neutron-server.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 08:45:52 EDT; 24s ago Main PID: 68051 (neutron-server) CGroup: /system.slice/neutron-server.service ├─68051 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ... ├─70718 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ... ├─70719 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ... ├─70720 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ... ├─70721 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ... ├─70722 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ... ├─70723 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ... └─70724 /usr/bin/python2 /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf ...Sep 24 08:44:30 controller01 systemd[1]: Starting OpenStack Neutron Server...Sep 24 08:45:53 controller01 systemd[1]: Started OpenStack Neutron Server.● neutron-linuxbridge-agent.service - OpenStack Neutron Linux Bridge Agent Loaded: loaded (/usr/lib/systemd/system/neutron-linuxbridge-agent.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 08:44:31 EDT; 1min 45s ago Process: 68052 ExecStartPre=/usr/bin/neutron-enable-bridge-firewall.sh (code=exited, status=0/SUCCESS) Main PID: 68125 (neutron-linuxbr) CGroup: /system.slice/neutron-linuxbridge-agent.service ├─68125 /usr/bin/python2 /usr/bin/neutron-linuxbridge-agent --config-file /usr/share/neutron/neutron... ├─69487 sudo neutron-rootwrap-daemon /etc/neutron/rootwrap.conf └─69584 /usr/bin/python2 /usr/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.confSep 24 08:44:31 controller01 systemd[1]: Starting OpenStack Neutron Linux Bridge Agent...Sep 24 08:44:31 controller01 neutron-enable-bridge-firewall.sh[68052]: net.bridge.bridge-nf-call-iptables = 1Sep 24 08:44:31 controller01 systemd[1]: Started OpenStack Neutron Linux Bridge Agent.Sep 24 08:45:21 controller01 sudo[69487]: neutron : TTY=unknown ; PWD=/ ; USER=root ; COMMAND=/bin/neutro....conf● neutron-dhcp-agent.service - OpenStack Neutron DHCP Agent Loaded: loaded (/usr/lib/systemd/system/neutron-dhcp-agent.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 08:44:31 EDT; 1min 46s ago Main PID: 68053 (neutron-dhcp-ag) CGroup: /system.slice/neutron-dhcp-agent.service └─68053 /usr/bin/python2 /usr/bin/neutron-dhcp-agent --config-file /usr/share/neutron/neutron-dist.c...Sep 24 08:44:31 controller01 systemd[1]: Started OpenStack Neutron DHCP Agent.Sep 24 08:44:31 controller01 systemd[1]: Starting OpenStack Neutron DHCP Agent...● neutron-metadata-agent.service - OpenStack Neutron Metadata Agent Loaded: loaded (/usr/lib/systemd/system/neutron-metadata-agent.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 08:44:31 EDT; 1min 46s ago Main PID: 68054 (neutron-metadat) CGroup: /system.slice/neutron-metadata-agent.service ├─68054 /usr/bin/python2 /usr/bin/neutron-metadata-agent --config-file /usr/share/neutron/neutron-di... ├─69448 /usr/bin/python2 /usr/bin/neutron-metadata-agent --config-file /usr/share/neutron/neutron-di... └─69449 /usr/bin/python2 /usr/bin/neutron-metadata-agent --config-file /usr/share/neutron/neutron-di...Sep 24 08:44:31 controller01 systemd[1]: Started OpenStack Neutron Metadata Agent.Sep 24 08:44:31 controller01 systemd[1]: Starting OpenStack Neutron Metadata Agent...● neutron-l3-agent.service - OpenStack Neutron Layer 3 Agent Loaded: loaded (/usr/lib/systemd/system/neutron-l3-agent.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 08:44:31 EDT; 1min 46s ago Main PID: 68055 (neutron-l3-agen) CGroup: /system.slice/neutron-l3-agent.service └─68055 /usr/bin/python2 /usr/bin/neutron-l3-agent --config-file /usr/share/neutron/neutron-dist.con...Sep 24 08:44:31 controller01 systemd[1]: Started OpenStack Neutron Layer 3 Agent.Sep 24 08:44:31 controller01 systemd[1]: Starting OpenStack Neutron Layer 3 Agent...Hint: Some lines were ellipsized, use -l to show in full. 配置compute节点安装组件yum install openstack-neutron-linuxbridge ebtables ipset 配置公共组件编辑/etc/neutron/neutron.conf[DEFAULT]auth_strategy = keystonetransport_url = rabbit://openstack:123456@controller[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 配置网络配置Linux网桥编辑 /etc/neutron/plugins/ml2/linuxbridge_agent.ini[linux_bridge]physical_interface_mappings = provider: ens38[vxlan]enable_vxlan = truelocal_ip = 10.10.10.20l2_population = true[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 配置计算节点网络服务编辑/etc/nova/nova.conf[neutron]url = http://controller:9696auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456 完成安装重启compute服务systemctl restart openstack-nova-compute.service 设置网桥服务开机启动systemctl enable neutron-linuxbridge-agent.servicesystemctl start neutron-linuxbridge-agent.service[root@compute ~]# systemctl restart openstack-nova-compute.service[root@compute ~]# systemctl status openstack-nova-compute.service● openstack-nova-compute.service - OpenStack Nova Compute Server Loaded: loaded (/usr/lib/systemd/system/openstack-nova-compute.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 09:05:45 EDT; 9s ago Main PID: 64952 (nova-compute) Tasks: 22 CGroup: /system.slice/openstack-nova-compute.service └─64952 /usr/bin/python2 /usr/bin/nova-computeSep 24 09:05:41 compute systemd[1]: Starting OpenStack Nova Compute Server...Sep 24 09:05:43 compute nova-compute[64952]: /usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefaca...rtedSep 24 09:05:43 compute nova-compute[64952]: exception.NotSupportedWarningSep 24 09:05:45 compute systemd[1]: Started OpenStack Nova Compute Server.Hint: Some lines were ellipsized, use -l to show in full.[root@compute ~]# systemctl enable neutron-linuxbridge-agent.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/neutron-linuxbridge-agent.service to /usr/lib/systemd/system/neutron-linuxbridge-agent.service.[root@compute ~]# systemctl start neutron-linuxbridge-agent.service[root@compute ~]# systemctl status neutron-linuxbridge-agent.service● neutron-linuxbridge-agent.service - OpenStack Neutron Linux Bridge Agent Loaded: loaded (/usr/lib/systemd/system/neutron-linuxbridge-agent.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 09:06:27 EDT; 5s ago Process: 65951 ExecStartPre=/usr/bin/neutron-enable-bridge-firewall.sh (code=exited, status=0/SUCCESS) Main PID: 65960 (neutron-linuxbr) Tasks: 6 CGroup: /system.slice/neutron-linuxbridge-agent.service ├─65960 /usr/bin/python2 /usr/bin/neutron-linuxbridge-agent --config-file /usr/share/neutron/neutron... ├─66027 sudo neutron-rootwrap-daemon /etc/neutron/rootwrap.conf └─66028 /usr/bin/python2 /usr/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.confSep 24 09:06:26 compute systemd[1]: Starting OpenStack Neutron Linux Bridge Agent...Sep 24 09:06:27 compute neutron-enable-bridge-firewall.sh[65951]: net.bridge.bridge-nf-call-iptables = 1Sep 24 09:06:27 compute neutron-enable-bridge-firewall.sh[65951]: net.bridge.bridge-nf-call-ip6tables = 1Sep 24 09:06:27 compute systemd[1]: Started OpenStack Neutron Linux Bridge Agent.Sep 24 09:06:29 compute sudo[66027]: neutron : TTY=unknown ; PWD=/ ; USER=root ; COMMAND=/bin/neutron-roo....confHint: Some lines were ellipsized, use -l to show in full.]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack</tag>
        <tag>neutron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(六)-Compute服务部署]]></title>
    <url>%2F2018%2F06%2F24%2FOpenStack%2F2018-06-24-openstack_queens%E5%AE%9E%E6%88%98(%E5%85%AD)%2F</url>
    <content type="text"><![CDATA[Compute服务介绍安装和配置controller节点前提条件在你安装和配置计算服务前，你必须创建数据库，服务凭证和API端点。 初始化Nova数据库# 创建nova_api, nova, nova_cell0数据库mysql -u root -pCREATE DATABASE nova_api;CREATE DATABASE nova;CREATE DATABASE nova_cell0;# 数据库登录授权GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY '123456'; 创建nova用户[root@controller01 ~]# source admin-openrc.sh [root@controller01 ~]# openstack user create --domain default --password-prompt novaUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 24e439911902457593f1126e68822c12 || name | nova || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+ 添加admin用户为nova用户openstack role add --project service --user nova admin 创建nova服务端点[root@controller01 ~]# openstack service create --name nova --description "OpenStack Compute" compute+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | OpenStack Compute || enabled | True || id | 751318d9b700403ebcc770e8d666c677 || name | nova || type | compute |+-------------+----------------------------------+ 创建compute API 服务端点[root@controller01 ~]# openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 250176341c0246a08a60d5f07b5f09f6 || interface | public || region | RegionOne || region_id | RegionOne || service_id | 751318d9b700403ebcc770e8d666c677 || service_name | nova || service_type | compute || url | http://controller:8774/v2.1 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | b051ac6bb1df44d9b2acd4b8d166317b || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 751318d9b700403ebcc770e8d666c677 || service_name | nova || service_type | compute || url | http://controller:8774/v2.1 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | bf49f639c87d4d3082316ba8a1f08dca || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 751318d9b700403ebcc770e8d666c677 || service_name | nova || service_type | compute || url | http://controller:8774/v2.1 |+--------------+----------------------------------+ 创建一个placement服务用户[root@controller01 ~]# openstack user create --domain default --password-prompt placementUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 538b20632a5342b2a455a55b4fce9427 || name | placement || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+ 添加placement用户为项目服务admin角色openstack role add --project service --user placement admin 创建在服务目录创建Placement API服务[root@controller01 ~]# openstack service create --name placement --description "Placement API" placement+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Placement API || enabled | True || id | 717638f8d4434d8dbbb31beff29ba8cb || name | placement || type | placement |+-------------+----------------------------------+ 创建Placement API服务端点[root@controller01 ~]# openstack endpoint create --region RegionOne placement public http://controller:8778+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 01bdda964bdb4591b86efddfbd1a2509 || interface | public || region | RegionOne || region_id | RegionOne || service_id | 717638f8d4434d8dbbb31beff29ba8cb || service_name | placement || service_type | placement || url | http://controller:8778 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne placement internal http://controller:8778+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 7254d04cb291495290fc3dd92ef69c8d || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 717638f8d4434d8dbbb31beff29ba8cb || service_name | placement || service_type | placement || url | http://controller:8778 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne placement admin http://controller:8778+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 40827ab720ee4782879ccdbe91ff058b || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 717638f8d4434d8dbbb31beff29ba8cb || service_name | placement || service_type | placement || url | http://controller:8778 |+--------------+----------------------------------+ 安装和配置组件安装软件包yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler python-novaclient openstack-nova-cert openstack-nova-placement-api 编辑 /etc/nova/nova.conf[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@controllermy_ip = 10.71.11.12use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[api_database]connection = mysql+pymysql://nova:123456@controller/nova_api[database]connection = mysql+pymysql://nova:123456@controller/nova[api]auth_strategy = keystone[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled = trueserver_listen = $my_ipserver_proxyclient_address = $my_ip[glance]api_servers = http://controller:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]os_region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:35357/v3username = placementpassword = 123456 配置/etc/httpd/conf.d/# 由于软件包的一个bug，需要在/etc/httpd/conf.d/00-nova-placement-api.conf文件中添加如下配置&lt;Directory /usr/bin&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt;&lt;/Directory&gt; 重新http服务systemctl restart httpd 同步nova-api数据库su -s /bin/sh -c &quot;nova-manage api_db sync&quot; nova 注册cell0数据库su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova[root@controller ~]# su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova 创建cell1 cell[root@controller01 ~]# su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova8d5f2513-90c6-452b-b889-9caa7ddb2907 同步nova数据库[root@controller01 ~]# su -s /bin/sh -c "nova-manage db sync" nova/usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `block_device_mapping_instance_uuid_virtual_name_device_name_idx`. This is deprecated and will be disallowed in a future release.') result = self._query(query)/usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `uniq_instances0uuid`. This is deprecated and will be disallowed in a future release.') result = self._query(query) 验证 nova、 cell0、 cell1数据库是否注册正确[root@controller01 ~]# nova-manage cell_v2 list_cells+-------+--------------------------------------+------------------------------------+-------------------------------------------------+| Name | UUID | Transport URL | Database Connection |+-------+--------------------------------------+------------------------------------+-------------------------------------------------+| cell0 | 00000000-0000-0000-0000-000000000000 | none:/ | mysql+pymysql://nova:****@controller/nova_cell0 || cell1 | 8d5f2513-90c6-452b-b889-9caa7ddb2907 | rabbit://openstack:****@controller | mysql+pymysql://nova:****@controller/nova |+-------+--------------------------------------+------------------------------------+-------------------------------------------------+ 设置服务为开机启动[root@contorller ~]# systemctl enable openstack-nova-api openstack-nova-conductor openstack-nova-consoleauth openstack-nova-novncproxy[root@contorller ~]# systemctl enable openstack-nova-scheduler[root@controller01 ~]# systemctl start openstack-nova-api openstack-nova-conductor openstack-nova-consoleauth openstack-nova-novncproxy openstack-nova-scheduler[root@controller01 ~]# systemctl status openstack-nova-api openstack-nova-conductor openstack-nova-consoleauth openstack-nova-novncproxy openstack-nova-scheduler● openstack-nova-api.service - OpenStack Nova API Server Loaded: loaded (/usr/lib/systemd/system/openstack-nova-api.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 04:30:27 EDT; 16s ago Main PID: 104847 (nova-api) CGroup: /system.slice/openstack-nova-api.service ├─104847 /usr/bin/python2 /usr/bin/nova-api ├─105293 /usr/bin/python2 /usr/bin/nova-api ├─105294 /usr/bin/python2 /usr/bin/nova-api ├─105295 /usr/bin/python2 /usr/bin/nova-api ├─105296 /usr/bin/python2 /usr/bin/nova-api ├─105311 /usr/bin/python2 /usr/bin/nova-api ├─105312 /usr/bin/python2 /usr/bin/nova-api ├─105313 /usr/bin/python2 /usr/bin/nova-api └─105314 /usr/bin/python2 /usr/bin/nova-apiSep 24 04:30:10 controller01 systemd[1]: Starting OpenStack Nova API Server...Sep 24 04:30:28 controller01 systemd[1]: Started OpenStack Nova API Server.● openstack-nova-conductor.service - OpenStack Nova Conductor Server Loaded: loaded (/usr/lib/systemd/system/openstack-nova-conductor.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 04:30:15 EDT; 29s ago Main PID: 104848 (nova-conductor) CGroup: /system.slice/openstack-nova-conductor.service ├─104848 /usr/bin/python2 /usr/bin/nova-conductor ├─105085 /usr/bin/python2 /usr/bin/nova-conductor ├─105086 /usr/bin/python2 /usr/bin/nova-conductor ├─105087 /usr/bin/python2 /usr/bin/nova-conductor └─105088 /usr/bin/python2 /usr/bin/nova-conductorSep 24 04:30:10 controller01 systemd[1]: Starting OpenStack Nova Conductor Server...Sep 24 04:30:15 controller01 systemd[1]: Started OpenStack Nova Conductor Server.● openstack-nova-consoleauth.service - OpenStack Nova VNC console auth Server Loaded: loaded (/usr/lib/systemd/system/openstack-nova-consoleauth.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 04:30:14 EDT; 30s ago Main PID: 104849 (nova-consoleaut) CGroup: /system.slice/openstack-nova-consoleauth.service └─104849 /usr/bin/python2 /usr/bin/nova-consoleauthSep 24 04:30:10 controller01 systemd[1]: Starting OpenStack Nova VNC console auth Server...Sep 24 04:30:14 controller01 systemd[1]: Started OpenStack Nova VNC console auth Server.● openstack-nova-novncproxy.service - OpenStack Nova NoVNC Proxy Server Loaded: loaded (/usr/lib/systemd/system/openstack-nova-novncproxy.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 04:30:10 EDT; 34s ago Main PID: 104850 (nova-novncproxy) CGroup: /system.slice/openstack-nova-novncproxy.service └─104850 /usr/bin/python2 /usr/bin/nova-novncproxy --web /usr/share/novnc/Sep 24 04:30:10 controller01 systemd[1]: Started OpenStack Nova NoVNC Proxy Server.Sep 24 04:30:10 controller01 systemd[1]: Starting OpenStack Nova NoVNC Proxy Server...● openstack-nova-scheduler.service - OpenStack Nova Scheduler Server Loaded: loaded (/usr/lib/systemd/system/openstack-nova-scheduler.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2018-09-24 04:30:15 EDT; 29s ago Main PID: 104851 (nova-scheduler) CGroup: /system.slice/openstack-nova-scheduler.service └─104851 /usr/bin/python2 /usr/bin/nova-schedulerSep 24 04:30:10 controller01 systemd[1]: Starting OpenStack Nova Scheduler Server...Sep 24 04:30:15 controller01 systemd[1]: Started OpenStack Nova Scheduler Server. 验证服务端口root@controller01 ~]# netstat -tunlp | egrep '8774|8775|8778|6080'tcp 0 0 0.0.0.0:6080 0.0.0.0:* LISTEN 104850/python2 tcp 0 0 0.0.0.0:8774 0.0.0.0:* LISTEN 104847/python2 tcp 0 0 0.0.0.0:8775 0.0.0.0:* LISTEN 104847/python2 tcp6 0 0 :::8778 :::* LISTEN 39993/httpd [root@controller01 ~]# openstack service list+----------------------------------+-----------+-----------+| ID | Name | Type |+----------------------------------+-----------+-----------+| 5e14d78760624dd9b034340473c98b32 | glance | image || 717638f8d4434d8dbbb31beff29ba8cb | placement | placement || 751318d9b700403ebcc770e8d666c677 | nova | compute || 99b27d6f304c4394a96804b0b31b57c3 | keystone | identity |+----------------------------------+-----------+-----------+[root@controller01 ~]# openstack service list --long+----------------------------------+-----------+-----------+-------------------+---------+| ID | Name | Type | Description | Enabled |+----------------------------------+-----------+-----------+-------------------+---------+| 5e14d78760624dd9b034340473c98b32 | glance | image | OpenStack Image | True || 717638f8d4434d8dbbb31beff29ba8cb | placement | placement | Placement API | True || 751318d9b700403ebcc770e8d666c677 | nova | compute | OpenStack Compute | True || 99b27d6f304c4394a96804b0b31b57c3 | keystone | identity | | True |+----------------------------------+-----------+-----------+-------------------+---------+ 安装和配置compute节点安装软件包yum install openstack-nova-compute 编辑/etc/nova/nova.conf[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@controllermy_ip = 10.10.10.10use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[api]auth_strategy = keystone[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled = Trueserver_listen = 0.0.0.0server_proxyclient_address = $my_ipnovncproxy_base_url = http://controller:6080/vnc_auto.html[glance]api_servers = http://controller:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]os_region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:35357/v3username = placementpassword = 123456 设置服务开机启动systemctl enable libvirtd.service openstack-nova-compute.servicesystemctl start libvirtd.service openstack-nova-compute.service 验证controller节点验证计算服务操作添加计算节点[root@controller01 ~]# su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" novaFound 3 cell mappings.Skipping cell0 since it does not contain hosts.Getting computes from cell 'cell1': 8d5f2513-90c6-452b-b889-9caa7ddb2907Checking host mapping for compute host 'compute': 996ee2bf-e930-4bfc-913b-c9fdc4a77f2eFound 0 unmapped computes in cell: 8d5f2513-90c6-452b-b889-9caa7ddb2907Getting computes from cell: bef99848-7d08-458e-b444-83356f703811Checking host mapping for compute host 'compute': 996ee2bf-e930-4bfc-913b-c9fdc4a77f2eFound 0 unmapped computes in cell: bef99848-7d08-458e-b444-83356f703811[root@controller01 ~]# openstack hypervisor list+----+---------------------+-----------------+-------------+-------+| ID | Hypervisor Hostname | Hypervisor Type | Host IP | State |+----+---------------------+-----------------+-------------+-------+| 1 | compute | QEMU | 10.10.10.10 | up |+----+---------------------+-----------------+-------------+-------+ 列出服务组件[root@contorller openstack]# source admin-openrc.sh [root@contorller openstack]# openstack compute service list+----+------------------+------------+----------+---------+-------+----------------------------+| ID | Binary | Host | Zone | Status | State | Updated At |+----+------------------+------------+----------+---------+-------+----------------------------+| 3 | nova-conductor | contorller | internal | enabled | up | 2018-06-24T07:02:27.000000 || 4 | nova-console | contorller | internal | enabled | up | 2018-06-23T16:20:04.000000 || 5 | nova-scheduler | contorller | internal | enabled | up | 2018-06-24T07:02:28.000000 || 6 | nova-consoleauth | contorller | internal | enabled | up | 2018-06-23T16:20:11.000000 || 7 | nova-compute | compute | nova | enabled | up | 2018-06-24T07:02:27.000000 |+----+------------------+------------+----------+---------+-------+----------------------------+[root@controller01 ~]# openstack compute service list+----+------------------+--------------+----------+---------+-------+----------------------------+| ID | Binary | Host | Zone | Status | State | Updated At |+----+------------------+--------------+----------+---------+-------+----------------------------+| 1 | nova-consoleauth | controller01 | internal | enabled | up | 2018-09-24T09:44:02.000000 || 2 | nova-scheduler | controller01 | internal | enabled | up | 2018-09-24T09:44:01.000000 || 3 | nova-conductor | controller01 | internal | enabled | up | 2018-09-24T09:44:01.000000 || 9 | nova-compute | compute | nova | enabled | up | 2018-09-24T09:44:09.000000 |+----+------------------+--------------+----------+---------+-------+----------------------------+ 列出身份服务中的API端点以验证与身份服务的连接[root@controller01 ~]# openstack catalog list+-----------+-----------+-----------------------------------------+| Name | Type | Endpoints |+-----------+-----------+-----------------------------------------+| glance | image | RegionOne || | | public: http://controller:9292 || | | RegionOne || | | admin: http://controller:9292 || | | RegionOne || | | internal: http://controller:9292 || | | || placement | placement | RegionOne || | | public: http://controller:8778 || | | RegionOne || | | admin: http://controller:8778 || | | RegionOne || | | internal: http://controller:8778 || | | || nova | compute | RegionOne || | | public: http://controller:8774/v2.1 || | | RegionOne || | | internal: http://controller:8774/v2.1 || | | RegionOne || | | admin: http://controller:8774/v2.1 || | | || keystone | identity | RegionOne || | | public: http://controller:5000/v3/ || | | RegionOne || | | admin: http://controller:35357/v3/ || | | RegionOne || | | internal: http://controller:5000/v3/ || | | |+-----------+-----------+-----------------------------------------+[root@controller ~]# openstack image list+--------------------------------------+--------+--------+| ID | Name | Status |+--------------------------------------+--------+--------+| 916faa2b-e292-46e0-bfe4-0f535069a1a0 | cirros | active |+--------------------------------------+--------+--------+ 检查cells和placement API是否正常[root@controller ~]# nova-status upgrade checkURL mysql://nova:***@localhost/nova_api does not contain a '+drivername' portion, and will make use of a default driver. A full dbname+drivername:// protocol is recommended. For MySQL, it is strongly recommended that mysql+pymysql:// be specified for maximum service compatibilityOption "os_region_name" from group "placement" is deprecated. Use option "region-name" from group "placement".URL mysql://nova:***@controller/nova does not contain a '+drivername' portion, and will make use of a default driver. A full dbname+drivername:// protocol is recommended. For MySQL, it is strongly recommended that mysql+pymysql:// be specified for maximum service compatibilityURL mysql://nova:***@controller/nova does not contain a '+drivername' portion, and will make use of a default driver. A full dbname+drivername:// protocol is recommended. For MySQL, it is strongly recommended that mysql+pymysql:// be specified for maximum service compatibilityURL mysql://nova:***@controller/nova_cell0 does not contain a '+drivername' portion, and will make use of a default driver. A full dbname+drivername:// protocol is recommended. For MySQL, it is strongly recommended that mysql+pymysql:// be specified for maximum service compatibility[root@controller01 ~]# nova-status upgrade check+------------------------------------------------------------------+| Upgrade Check Results |+------------------------------------------------------------------+| Check: Cells v2 || Result: Failure || Details: No host mappings found but there are compute nodes. Run || command 'nova-manage cell_v2 simple_cell_setup' and then || retry. |+------------------------------------------------------------------+| Check: Placement API || Result: Success || Details: None |+------------------------------------------------------------------+| Check: Resource Providers || Result: Success || Details: None |+------------------------------------------------------------------+| Check: Ironic Flavor Migration || Result: Success || Details: None |+------------------------------------------------------------------+| Check: API Service Version || Result: Success || Details: None |+------------------------------------------------------------------+[root@controller01 ~]# nova-manage cell_v2 simple_cell_setupCell0 is already setup[root@controller01 ~]# nova-status upgrade check+--------------------------------+| Upgrade Check Results |+--------------------------------+| Check: Cells v2 || Result: Success || Details: None |+--------------------------------+| Check: Placement API || Result: Success || Details: None |+--------------------------------+| Check: Resource Providers || Result: Success || Details: None |+--------------------------------+| Check: Ironic Flavor Migration || Result: Success || Details: None |+--------------------------------+| Check: API Service Version || Result: Success || Details: None |+--------------------------------+ 部署错误同步nova-api数据库源码BUG报错：/usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:332: NotSupportedWarning: Configuration option(s) ['use_tpool'] not supported exception.NotSupportedWarning 修复https://github.com/openstack/oslo.db/commit/c432d9e93884d6962592f6d19aaec3f8f66ac3a2 配置文件错误报错日志：2018-06-23 22:57:55.777 35210 ERROR oslo.messaging._drivers.impl_rabbit [req-d970f090-389a-4916-8164-210e6f7391d8 - - - - -] Unable to connect to AMQP server on controller:5672 after None tries: (0, 0): (403) ACCESS_REFUSED - Login was refused using authentication mechanism AMQPLAIN. For details see the broker logfile.: AccessRefused: (0, 0): (403) ACCESS_REFUSED - Login was refused using authentication mechanism AMQPLAIN. For details see the broker logfile. Controller 中nova配置密码错误，修改配置文件后解决。 [root@controller01 ~]# openstack compute service list --service nova-compute+----+--------------+---------+------+---------+-------+----------------------------+| ID | Binary | Host | Zone | Status | State | Updated At |+----+--------------+---------+------+---------+-------+----------------------------+| 9 | nova-compute | compute | nova | enabled | up | 2018-09-24T09:42:19.000000 |+----+--------------+---------+------+---------+-------+----------------------------+]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(五)-Horizon服务部署]]></title>
    <url>%2F2018%2F06%2F22%2FOpenStack%2F2018-06-23-openstack_queens%E5%AE%9E%E6%88%98(%E4%BA%94)%2F</url>
    <content type="text"><![CDATA[Horizon服务介绍OpenStack Dashboard，同样作为horizon &lt;https://git.openstack.org/cgit/openstack/horizon&gt;为人所知是一个web接口，使得云管理员和用户可以管理不同的OpenStack资源和服务。仪表盘使得通过OpenStack API与OpenStack计算云控制器进行基于web的交互成为可能。Horizon 允许您自定义仪表板的商标。Horizon 提供了一套内核类和可重复使用的模板及工具。这个部署使用的是 Apache Web （http）服务器。 服务安装安装软件包yum install openstack-dashboard -y 编辑/etc/openstack-dashboard/local_settingsOPENSTACK_HOST = "controller"ALLOWED_HOSTS = ['*']配置memcache会话存储SESSION_ENGINE = 'django.contrib.sessions.backends.cache'CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'controller:11211', &#125;&#125;开启身份认证API 版本v3OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HO开启domains版本支持OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True配置API版本OPENSTACK_API_VERSIONS = &#123; "identity": 3, "image": 2, "volume": 2,&#125;OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"：OPENSTACK_NEUTRON_NETWORK = &#123; 'enable_router': False, 'enable_quotas': False, 'enable_distributed_router': False, 'enable_ha_router': False, 'enable_lb': False, 'enable_firewall': False, 'enable_vpn': False, 'enable_fip_topology_check': False,&#125; 完成安装，重启web服务和会话存储systemctl restart httpd.service memcached.service在浏览器输入http://10.10.10.10/dashboard.,访问openstack的web页面defaultadmin123456]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack</tag>
        <tag>horizon</tag>
        <tag>dashboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(四)-Glance服务部署]]></title>
    <url>%2F2018%2F06%2F22%2FOpenStack%2F2018-06-22-openstack_queens%E5%AE%9E%E6%88%98(%E5%9B%9B)%2F</url>
    <content type="text"><![CDATA[glance服务OpenStack 的镜像服务 (glance) 允许用户发现、注册和恢复虚拟机镜像。它提供了一个 REST API，允许您查询虚拟机镜像的 metadata 并恢复一个实际的镜像。您可以存储虚拟机镜像通过不同位置的镜像服务使其可用，就像 OpenStack 对象存储那样从简单的文件系统到对象存储系统。 前提条件安装和配置镜像服务之前，你必须创建创建一个数据库、服务凭证和API端点。 配置glance服务数据库创建glance数据库,并授权 mysql -u root -pCREATE DATABASE glance;GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY '123456'; 获取admin环境变量source admin-openrc.sh 创建服务凭据创建glance用户[root@controller01 ~]# openstack user create --domain default --password-prompt glanceUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | a967e74b216746a68121512d5797dd3b || name | glance || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+ 把admin用户添加到glance用户和项目中openstack role add --project service --user glance admin 创建glance服务[root@controller01 ~]# openstack service create --name glance --description "OpenStack Image" image+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | OpenStack Image || enabled | True || id | 5e14d78760624dd9b034340473c98b32 || name | glance || type | image |+-------------+----------------------------------+ 创建镜像服务API端点[root@controller01 ~]# openstack endpoint create --region RegionOne image public http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 6c8b29aa5884482d9d69658705e80b39 || interface | public || region | RegionOne || region_id | RegionOne || service_id | 5e14d78760624dd9b034340473c98b32 || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne image internal http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | b4b9fead99d9420e89ffc9ef03a585d0 || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 5e14d78760624dd9b034340473c98b32 || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+[root@controller01 ~]# openstack endpoint create --region RegionOne image admin http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 9c6f75cf3c8c442fa209e6cddde1073c || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 5e14d78760624dd9b034340473c98b32 || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+ 安装并配置组件安装软件包yum install openstack-glance python-glance python-glanceclient 编辑/etc/glance/glance-api.conf文件#在 [database] 部分，配置数据库访问：[database]connection = mysql+pymysql://glance:123456@controller/glance#在 [keystone_authtoken] 和 [paste_deploy] 部分，配置认证服务访问：[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone#在 [glance_store]部分，配置本地文件系统存储和镜像文件位置：[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ 编辑/etc/glance/glance-registry.conf[database]connection = mysql+pymysql://glance:123456@controller/glance[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 同步镜像服务数据库#su -s /bin/sh -c "glance-manage db_sync" glance[root@controller01~]]# su -s /bin/sh -c "glance-manage db_sync" glance/usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:1336: OsloDBDeprecationWarning: EngineFacade is deprecated; please use oslo_db.sqlalchemy.enginefacade expire_on_commit=expire_on_commit, _conf=conf)INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; liberty, liberty initialINFO [alembic.runtime.migration] Running upgrade liberty -&gt; mitaka01, add index on created_at and updated_at columns of 'images' tableINFO [alembic.runtime.migration] Running upgrade mitaka01 -&gt; mitaka02, update metadef os_nova_serverINFO [alembic.runtime.migration] Running upgrade mitaka02 -&gt; ocata_expand01, add visibility to imagesINFO [alembic.runtime.migration] Running upgrade ocata_expand01 -&gt; pike_expand01, empty expand for symmetry with pike_contract01INFO [alembic.runtime.migration] Running upgrade pike_expand01 -&gt; queens_expand01INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.Upgraded database to: queens_expand01, current revision(s): queens_expand01INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.Database migration is up to date. No migration needed.INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade mitaka02 -&gt; ocata_contract01, remove is_public from imagesINFO [alembic.runtime.migration] Running upgrade ocata_contract01 -&gt; pike_contract01, drop glare artifacts tablesINFO [alembic.runtime.migration] Running upgrade pike_contract01 -&gt; queens_contract01INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.Upgraded database to: queens_contract01, current revision(s): queens_contract01INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.Database is synced successfully. 验证数据库 [root@controller01 ~]# mysql -h 10.10.10.10 -uglance -p -e "use glance;show tables;"Enter password: +----------------------------------+| Tables_in_glance |+----------------------------------+| alembic_version || image_locations || image_members || image_properties || image_tags || images || metadef_namespace_resource_types || metadef_namespaces || metadef_objects || metadef_properties || metadef_resource_types || metadef_tags || migrate_version || task_info || tasks |+----------------------------------+ 设置自启和启动服务systemctl enable openstack-glance-api.service openstack-glance-registry.servicesystemctl start openstack-glance-api.service openstack-glance-registry.service 验证操作使用CirrOS验证Image服务的操作，这是一个小型Linux映像，可帮助您测试OpenStack部署。有关如何下载和构建映像的更多信息，请参阅OpenStack虚拟机映像指南https://docs.openstack.org/image-guide/有关如何管理映像的信息，请参阅OpenStack最终用户指南https://docs.openstack.org/queens/user/ 获取admin用户的环境变量，且下载镜像source admin-openrcwget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img 上传镜像使用QCOW2磁盘格式，裸容器格式和公开可见性将图像上传到Image服务，以便所有项目都可以访问它： [root@controller01 ~]# openstack image create "cirros" --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --public+------------------+------------------------------------------------------+| Field | Value |+------------------+------------------------------------------------------+| checksum | f8ab98ff5e73ebab884d80c9dc9c7290 || container_format | bare || created_at | 2018-09-24T07:04:21Z || disk_format | qcow2 || file | /v2/images/4ec21f62-a4b5-45d1-a37a-0a52e7b2f48e/file || id | 4ec21f62-a4b5-45d1-a37a-0a52e7b2f48e || min_disk | 0 || min_ram | 0 || name | cirros || owner | 74e2b02869264cb0a4f62298b68deb28 || protected | False || schema | /v2/schemas/image || size | 13267968 || status | active || tags | || updated_at | 2018-09-24T07:04:21Z || virtual_size | None || visibility | public |+------------------+------------------------------------------------------+[root@controller01 ~]# openstack image list+--------------------------------------+--------+--------+| ID | Name | Status |+--------------------------------------+--------+--------+| 4ec21f62-a4b5-45d1-a37a-0a52e7b2f48e | cirros | active |+--------------------------------------+--------+--------+ 配置过程排错配置文件错误[root@controller01~]]# su -s /bin/sh -c "glance-manage db_sync" glance/usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:1336: OsloDBDeprecationWarning: EngineFacade is deprecated; please use oslo_db.sqlalchemy.enginefacade expire_on_commit=expire_on_commit, _conf=conf) 查看日志2018-09-24 02:33:48.207 114517 CRITICAL glance [-] Unhandled error: ArgumentError: Could not parse rfc1738 URL from string &apos;connection = mysql+pymysql://glance:123456@controller/glance&apos; 错误为配置文件中数据连接有问题 connection = connection = mysql+pymysql://glance:123456@controller/glance 服务端口没打开[root@controller01 ~]# openstack image create "cirros" --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --publicError finding address for http://controller:9292/v2/schemas/image: HTTPConnectionPool(host='controller', port=9292): Max retries exceeded with url: /v2/schemas/image (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7fa2ebb138d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))[root@controller01 ~]# ss -ntl | grep 9292]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack,glance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(三)-keystone部署]]></title>
    <url>%2F2018%2F06%2F21%2FOpenStack%2F2018-06-21-openstack_queens%E5%AE%9E%E6%88%98(%E4%B8%89)%2F</url>
    <content type="text"><![CDATA[Keystone介绍Keystone(OpenStack Identity Service)是 OpenStack 框架中负责管理身份验证、服务规则和服务令牌功能的模块。用户访问资源需要验证用户的身份与权限，服务执行操作也需要进行权限检测，这些都需要通过 Keystone 来处理。Keystone类似一个服务总线， 或者说是整个Openstack框架的注册表， 其他服务通过keystone来注册其服务的Endpoint（服务访问的URL），任何服务之间相互的调用， 需要经过Keystone的身份验证， 来获得目标服务的Endpoint来找到目标服务。 Keystone部署生成管理员令牌生成一个随机值在初始的配置中作为管理员的令牌。[root@controller ~]# openssl rand -hex 10f52ea17a549659e3c797 配置Keystone数据库创建keystone数据库并授权mysql -uroot -pCREATE DATABASE keystone;GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY '123456';GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY '123456'; 安装Keystone组件yum install openstack-keystone httpd mod_wsgi -y 配置Keystone配置文件#修改keystone的配置文件 /etc/keystone/keystone.conf：vim /etc/keystone/keystone.conf#在``[DEFAULT]``部分，定义初始管理令牌的值：[DEFAULT]...admin_token = f52ea17a549659e3c797#使用前面步骤生成的随机数替换``ADMIN_TOKEN``值。[database]connection = mysql+pymysql://keystone:123456@controller/keystone[token]provider = fernet#[memcache]配置Memcached服务：[memcache]...servers = 10.10.10.10:11211#[revoke] 部分，配置SQL 回滚驱动:[revoke]...driver = sql#DEFAULT（可选的）为帮助排错，在 “[DEFAULT]”部分启用详细日志。[DEFAULT]...verbose = True 同步keystone数据库[root@controller ~]# su -s /bin/sh -c "keystone-manage db_sync" keystone#验证MariaDB [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || keystone || mysql || performance_schema |+--------------------+4 rows in set (0.01 sec)MariaDB [(none)]&gt; use keystone;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [keystone]&gt; show tables;+-----------------------------+| Tables_in_keystone |+-----------------------------+| access_token || application_credential || application_credential_role || assignment || config_register || consumer || credential || endpoint || endpoint_group || federated_user || federation_protocol || group || id_mapping || identity_provider || idp_remote_ids || implied_role || limit || local_user || mapping || migrate_version || nonlocal_user || password || policy || policy_association || project || project_endpoint || project_endpoint_group || project_tag || region || registered_limit || request_token || revocation_event || role || sensitive_config || service || service_provider || system_assignment || token || trust || trust_role || user || user_group_membership || user_option || whitelisted_config |+-----------------------------+44 rows in set (0.00 sec) 初始化 Fernetkey[root@controller ~]# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone[root@controller ~]# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone#查看证书[root@controller ~]# ll /etc/keystone/total 140-rw-r-----. 1 root keystone 2303 Jul 27 03:38 default_catalog.templatesdrwx------. 2 keystone keystone 24 Sep 15 06:57 fernet-keys-rw-r-----. 1 root keystone 122856 Sep 15 06:45 keystone.conf-rw-r-----. 1 root keystone 2493 Jul 27 03:38 keystone-paste.ini-rw-r-----. 1 root keystone 1046 Jul 27 03:38 logging.conf-rw-r-----. 1 root keystone 3 Aug 2 02:21 policy.json-rw-r-----. 1 keystone keystone 665 Jul 27 03:38 sso_callback_template.html[root@controller ~]# ll /etc/keystone/fernet-keys/total 8-rw-------. 1 keystone keystone 44 Sep 15 06:57 0-rw-------. 1 keystone keystone 44 Sep 15 06:57 1[root@controller ~]# more 00: No such file or directory[root@controller ~]# more /etc/keystone/fernet-keys/08GxTQWsdlHxs6ckI7Jdf4XclWgjW0fograAS0OF-rFE=[root@controller ~]# more /etc/keystone/fernet-keys/12CyPY4WKTxMDXrkF0DpXNezZRDKc_-PPJ6bVZNlANnc= 引导身份认证服务keystone-manage bootstrap --bootstrap-password 123456 \--bootstrap-admin-url http://controller:35357/v3/ \--bootstrap-internal-url http://controller:5000/v3/ \--bootstrap-public-url http://controller:5000/v3/ \--bootstrap-region-id RegionOne#执行结果[root@controller ~]# keystone-manage bootstrap --bootstrap-password 123456 --bootstrap-admin-url http://controller:35357/v3/ --bootstrap-internal-url http://controller:5000/v3/ --bootstrap-public-url http://controller:5000/v3/ --bootstrap-region-id RegionOne 配置Apache http服务配置http配置文件[root@controller ~]# vim /etc/httpd/conf/httpd.conf ServerName 10.10.10.10:80 创建到/usr/share/keystone/wsgi-keystone.conf文件的链接[root@controller ~]# ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 重启Http服务[root@controller ~]# systemctl restart httpd[root@controller ~]# systemctl status httpd● httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; vendor preset: disabled) Active: active (running) since Sat 2018-09-15 09:33:10 EDT; 3s ago Docs: man:httpd(8) man:apachectl(8) Process: 42167 ExecStop=/bin/kill -WINCH $&#123;MAINPID&#125; (code=exited, status=0/SUCCESS) Main PID: 42204 (httpd) Status: "Processing requests..." CGroup: /system.slice/httpd.service ├─42204 /usr/sbin/httpd -DFOREGROUND ├─42205 (wsgi:keystone- -DFOREGROUND ├─42206 (wsgi:keystone- -DFOREGROUND ├─42207 (wsgi:keystone- -DFOREGROUND ├─42208 (wsgi:keystone- -DFOREGROUND ├─42209 (wsgi:keystone- -DFOREGROUND ├─42210 (wsgi:keystone- -DFOREGROUND ├─42211 (wsgi:keystone- -DFOREGROUND ├─42212 (wsgi:keystone- -DFOREGROUND ├─42213 (wsgi:keystone- -DFOREGROUND ├─42214 (wsgi:keystone- -DFOREGROUND ├─42215 /usr/sbin/httpd -DFOREGROUND ├─42241 /usr/sbin/httpd -DFOREGROUND ├─42247 /usr/sbin/httpd -DFOREGROUND ├─42248 /usr/sbin/httpd -DFOREGROUND └─42249 /usr/sbin/httpd -DFOREGROUND[root@controller ~]# netstat -tunpl|egrep -w "5000|35357"tcp6 0 0 :::35357 :::* LISTEN 42204/httpd tcp6 0 0 :::5000 :::* LISTEN 42204/httpd 创建项目和用户配置administrative账户环境变量export OS_USERNAME=adminexport OS_PASSWORD=123456export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_DOMAIN_NAME=Default #配置端点URLexport OS_AUTH_URL=http://controller:35357/v3 #配置认证API版本export OS_IDENTITY_API_VERSION=3 #管理员令牌export OS_TOKEN=9ce65bc1554d1f76f57e 创建域[root@controller01 ~]# openstack domain create --description "Domain" example+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Domain || enabled | True || id | c51c0aa6e57341dfb4a02f18c52cf09d || name | example || tags | [] |+-------------+----------------------------------+[root@controller01 ~]# openstack domain list+----------------------------------+---------+---------+--------------------+| ID | Name | Enabled | Description |+----------------------------------+---------+---------+--------------------+| c51c0aa6e57341dfb4a02f18c52cf09d | example | True | Domain || default | Default | True | The default domain |+----------------------------------+---------+---------+--------------------+ 创建Demo用户创建demo项目，添加到default域中[root@controller01 ~]# openstack project create --domain default --description "Demo Project" demo+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Demo Project || domain_id | default || enabled | True || id | ea196cb9d1b54bcc8f8c4c943543bfdf || is_domain | False || name | demo || parent_id | default || tags | [] |+-------------+----------------------------------+ 创建 demo 用户：（密码设为123456）[root@controller01 ~]# openstack user create --domain default --password-prompt demoUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 63564c05934743cebdab5dc912b611df || name | demo || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+ 创建 user 角色：（普通用户角色）[root@controller01 ~]# openstack role create demouser+-----------+----------------------------------+| Field | Value |+-----------+----------------------------------+| domain_id | None || id | 9514968895b54db492f0d9aa1febd1ba || name | demouser |+-----------+----------------------------------+ 添加 demouser 角色到 demo 项目和用户[root@controller01 ~]# openstack role add --project demo --user demo demouser 验证操作取消环境变量unset OS_AUTH_URL OS_PASSWORD admin用户返回的认证token[root@controller01 ~]# openstack --os-auth-url http://controller:35357/v3 \&gt; --os-project-domain-name Default --os-user-domain-name Default \&gt; --os-project-name admin --os-username admin token issuePassword: +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| expires | 2018-09-24T05:07:38+0000 || id | gAAAAABbqGMKyxE9-sejz6ZSszyVF_5TdR9hohZTmZpu7LufO5SLmY06ey6pmY3eIVA_l-bY0ps-Afzu6g4s3wgDtLsbb0Xuz8yM6vHihNu3_I4Er12GY5ql884yM1ss0i-Drz_FvcD_x9BVIKaeHHKH6KnpCRt5bcTefuHKS1FnHu9jn3YvoGo || project_id | 74e2b02869264cb0a4f62298b68deb28 || user_id | ddc847b9b5274108830404e90bf99bce |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ demo用户返回的认证token[root@controller01 ~]# openstack --os-auth-url http://controller:5000/v3 \&gt; --os-project-domain-name Default --os-user-domain-name Default \&gt; --os-project-name demo --os-username demo token issuePassword: +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| expires | 2018-09-24T05:09:40+0000 || id | gAAAAABbqGOEZ4G4EbmtLsvmAgjKBqxJX1ZfPsJ691V3sW2GpGP-rTAUKMIw55yk1q0_qIoVbjDDsjtVyCitKQVlxwW0_7Zr_KadINd8-TeRwqY4o1D_6qLH9l1Fds_UDlvCr87jWhoSdJvqH8EKcGfAAFc33l6eyiJ_G-n4R9ZBzvgR9WlknHo || project_id | ea196cb9d1b54bcc8f8c4c943543bfdf || user_id | 63564c05934743cebdab5dc912b611df |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 创建server服务[root@controller01 ~]# openstack project create --domain default --description "Service Project" service+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Service Project || domain_id | default || enabled | True || id | 45c3b7a0fe5f41c1adc11408c9231920 || is_domain | False || name | service || parent_id | default || tags | [] |+-------------+----------------------------------+ 创建脚本创建 admin 和 demo项目和用户创建客户端环境变量脚本。本实验的接下来的部分会引用这些脚本，为客户端操作加载合适的的凭证。 编辑文件 admin-openrc.sh 并添加如下内容：export OS_PROJECT_DOMAIN_ID=defaultexport OS_USER_DOMAIN_ID=defaultexport OS_PROJECT_NAME=adminexport OS_TENANT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=ADMIN_PASSexport OS_AUTH_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3 将 ADMIN_PASS 替换为你在认证服务中为 admin 用户选择的密码。 编辑文件 demo-openrc.sh 并添加如下内容：export OS_PROJECT_DOMAIN_ID=defaultexport OS_USER_DOMAIN_ID=defaultexport OS_PROJECT_NAME=demoexport OS_TENANT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=DEMO_PASSexport OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3 将 DEMO_PASS 替换为你在认证服务中为 demo 用户选择的密码。 使用脚本使用特定租户和用户运行客户端，你可以在运行之前简单地加载相关客户端脚本。例如：加载admin-openrc.sh文件来身份认证服务的环境变量位置和admin项目和用户证书： $ source admin-openrc.sh 请求认证令牌:[root@controller01 ~]# source admin-openrc.sh [root@controller01 ~]# openstack token issue+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| expires | 2018-09-24T05:16:23+0000 || id | gAAAAABbqGUXypq-OQgwKflKq52_Uw4DrIcz2Jb7BNQc451r3SYm9rNfPjI7kuNxm9Fg1vbiZKJV3w4Fj_nbbAkz-9W9o0LU8ObW0fwIAf0l4-ztiTDEI_kLuMwVnz_ltPHgKZe6mv6y6RSibO6sKP25UXt6sA2IgpW6glaKJUPQQmUcKwk-LkA || project_id | 74e2b02869264cb0a4f62298b68deb28 || user_id | ddc847b9b5274108830404e90bf99bce |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack</tag>
        <tag>keystone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(二)-Controller节点基础环境搭建]]></title>
    <url>%2F2018%2F06%2F20%2FOpenStack%2F2018-06-20-openstack_queens%E5%AE%9E%E6%88%98(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[SQL数据库大多数 OpenStack 服务使用 SQL 数据库来存储信息。 典型地，数据库运行在控制节点上。指南中的步骤依据不同的发行版使用MariaDB或 MySQL。OpenStack 服务也支持其他 SQL 数据库，包括PostgreSQL &lt;http://www.postgresql.org/&gt;。鉴于比较熟悉MySQL，本次实验使用Mariadb数据库。 安装数据库[root@localhost ~]# yum install mariadb mariadb-server MySQL-python 配置数据库[root@localhost ~]# vim /etc/my.cnf.d/mariadb-server.cnf #在 [mysqld] 部分，设置 ``bind-address``值为控制节点的管理网络IP地址以使得其它节点可以通过管理网络访问数据库：[mysqld]bind-address = 10.10.10.10#在``[mysqld]`` 部分，设置如下键值来启用一起有用的选项和 UTF-8 字符集[mysqld]default-storage-engine = innodbinnodb_file_per_table = onmax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 设置开机启动和启动服务[root@localhost ~]# systemctl enable mariadb.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/mariadb.service to /usr/lib/systemd/system/mariadb.service.[root@localhost ~]# systemctl start mariadb.service[root@localhost ~]# systemctl status mariadb● mariadb.service - MariaDB 10.1 database server Loaded: loaded (/usr/lib/systemd/system/mariadb.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2018-09-11 10:34:16 EDT; 15s ago Process: 49412 ExecStartPost=/usr/libexec/mysql-check-upgrade (code=exited, status=0/SUCCESS) Process: 48970 ExecStartPre=/usr/libexec/mysql-prepare-db-dir %n (code=exited, status=0/SUCCESS) Process: 48947 ExecStartPre=/usr/libexec/mysql-check-socket (code=exited, status=0/SUCCESS) Main PID: 49384 (mysqld) Status: &quot;Taking your SQL requests now...&quot; CGroup: /system.slice/mariadb.service └─49384 /usr/libexec/mysqld --basedir=/usrSep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: See the MariaDB Knowledgebase at http://mariadb.com/kb or theSep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: MySQL manual for more instructions.Sep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: Please report any problems at http://mariadb.org/jiraSep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: The latest information about MariaDB is available at http://mariadb.org/.Sep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: You can find additional information about the MySQL part at:Sep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: http://dev.mysql.comSep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: Consider joining MariaDB&apos;s strong and vibrant community:Sep 11 10:34:16 localhost.localdomain mysql-prepare-db-dir[48970]: https://mariadb.org/get-involved/Sep 11 10:34:16 localhost.localdomain mysqld[49384]: 2018-09-11 10:34:16 140412884986048 [Note] /usr/libexec/mysqld (mysqld 10.1.20-MariaDB) starting as pr...49384 ...Sep 11 10:34:16 localhost.localdomain systemd[1]: Started MariaDB 10.1 database server.Hint: Some lines were ellipsized, use -l to show in full. 初始化数据库[root@localhost ~]# mysql_secure_installationNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!In order to log into MariaDB to secure it, we'll need the currentpassword for the root user. If you've just installed MariaDB, andyou haven't set the root password yet, the password will be blank,so you should just press enter here.Enter current password for root (enter for none): OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MariaDBroot user without the proper authorisation.Set root password? [Y/n] yNew password: Re-enter new password: Password updated successfully!Reloading privilege tables.. ... Success!By default, a MariaDB installation has an anonymous user, allowing anyoneto log into MariaDB without having to have a user account created forthem. This is intended only for testing, and to make the installationgo a bit smoother. You should remove them before moving into aproduction environment.Remove anonymous users? [Y/n] n ... skipping.Normally, root should only be allowed to connect from 'localhost'. Thisensures that someone cannot guess at the root password from the network.Disallow root login remotely? [Y/n] n ... skipping.By default, MariaDB comes with a database named 'test' that anyone canaccess. This is also intended only for testing, and should be removedbefore moving into a production environment.Remove test database and access to it? [Y/n] n ... skipping.Reloading the privilege tables will ensure that all changes made so farwill take effect immediately.Reload privilege tables now? [Y/n] y ... Success!Cleaning up...All done! If you've completed all of the above steps, your MariaDBinstallation should now be secure.Thanks for using MariaDB! RabbitMQ消息队列OpenStack 使用 message queue 协调操作和各服务的状态信息。消息队列服务一般运行在控制节点上。OpenStack支持好几种消息队列服务包括 RabbitMQ, Qpid, 和 ZeroMQ。不过，大多数发行版本的OpenStack包支持特定的消息队列服务。本实验安装 RabbitMQ 消息队列服务，因为大部分发行版本都支持它。 RabbitMQ安装yum install rabbitmq-server -y 设置开机自启和启动服务#systemctl enable rabbitmq-server.service#systemctl start rabbitmq-server.service[root@controller ~]# systemctl enable rabbitmq-server.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/rabbitmq-server.service to /usr/lib/systemd/system/rabbitmq-server.service.[root@localhost ~]# systemctl start rabbitmq-server.service[root@localhost ~]# systemctl status rabbitmq-server.service● rabbitmq-server.service - RabbitMQ broker Loaded: loaded (/usr/lib/systemd/system/rabbitmq-server.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2018-09-14 09:02:30 EDT; 11s ago Main PID: 5248 (beam.smp) Status: "Initialized" CGroup: /system.slice/rabbitmq-server.service ├─5248 /usr/lib64/erlang/erts-8.3.5.3/bin/beam.smp -W w -A 64 -P 1048576 -t 5000000 -stbt db -K true -- -root /usr/lib64/erlang -progname erl -- -home /v... ├─5470 erl_child_setup 1024 ├─5496 inet_gethost 4 └─5497 inet_gethost 4Sep 14 09:02:27 localhost.localdomain systemd[1]: Starting RabbitMQ broker...Sep 14 09:02:29 localhost.localdomain rabbitmq-server[5248]: RabbitMQ 3.6.5. Copyright (C) 2007-2016 Pivotal Software, Inc.Sep 14 09:02:29 localhost.localdomain rabbitmq-server[5248]: ## ## Licensed under the MPL. See http://www.rabbitmq.com/Sep 14 09:02:29 localhost.localdomain rabbitmq-server[5248]: ## ##Sep 14 09:02:29 localhost.localdomain rabbitmq-server[5248]: ########## Logs: /var/log/rabbitmq/rabbit@localhost.logSep 14 09:02:29 localhost.localdomain rabbitmq-server[5248]: ###### ## /var/log/rabbitmq/rabbit@localhost-sasl.logSep 14 09:02:29 localhost.localdomain rabbitmq-server[5248]: ##########Sep 14 09:02:29 localhost.localdomain rabbitmq-server[5248]: Starting broker...Sep 14 09:02:30 localhost.localdomain systemd[1]: Started RabbitMQ broker.Sep 14 09:02:30 localhost.localdomain rabbitmq-server[5248]: completed with 0 plugins. 添加openstack 用户rabbitmqctl add_user openstack openstack[root@controller ~]# rabbitmqctl add_user openstack 123456Creating user "openstack" ... openstack用户的权限配置#rabbitmqctl set_user_tags openstack administrator#rabbitmqctl set_permissions openstack ".*" ".*" ".*"[root@controller etc]# rabbitmqctl set_user_tags openstack administratorSetting tags for user "openstack" to [administrator] ...[root@controller ~]# rabbitmqctl set_permissions openstack ".*" ".*" ".*"Setting permissions for user "openstack" in vhost "/" ... web界面登陆测试运行管理平台 http://10.10.10.10:15672 使用openstack/123456登陆即可。 如果IP地址加载失败，优先考虑是否运行管理平台[root@controller etc]# rabbitmq-plugins enable rabbitmq_managementThe following plugins have been enabled: mochiweb webmachine rabbitmq_web_dispatch amqp_client rabbitmq_management_agent rabbitmq_management Memcached缓存数据库服务的身份认证服务使用Memcached缓存令牌。 memcached服务通常在控制器节点上运行。 对于生产部署，建议启用防火墙，身份验证和加密的组合来保护它。 安装Memcachedyum install memcached python-memcached -y 配置Memcachedvi /etc/sysconfig/memcachedOPTIONS="-l 127.0.0.1,::1,controller" 设置开机自启和服务启动systemctl enable memcached.servicesystemctl start memcached.service 启动失败 配置文件错误，将 OPTIONS=&quot;-l 10.10.10.10,::1,controller&quot; 修改为OPTIONS=&quot;-l 127.0.0.1,::1,controller&quot; 验证Memcached服务[root@controller ~]# netstat -anltp|grep memcachetcp 0 0 172.16.14.224:11211 0.0.0.0: LISTEN 14940/memcached tcp 0 0 127.0.0.1:11211 0.0.0.0: LISTEN 14940/memcached tcp6 0 0 ::1:11211 :::* LISTEN 14940/memcached Etcd服务Etcd服务安装yum install etcd -y Etcd配置vim /etc/etcd/etcd.conf#[Member]ETCD_DATA_DIR="/var/lib/etcd/default.etcd"ETCD_LISTEN_PEER_URLS="http://110.10.10.10:2380"ETCD_LISTEN_CLIENT_URLS="http://10.10.10.10.224:2379"ETCD_NAME="controller"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.10.10.10:2380"ETCD_ADVERTISE_CLIENT_URLS="http://10.10.10.10:2379"ETCD_INITIAL_CLUSTER="controller=http://10.10.10.10:2380"ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-01"ETCD_INITIAL_CLUSTER_STATE="new" 开机自启和服务启动systemctl enable etcdsystemctl start etcd[root@controller ~]# vim /etc/etcd/etcd.conf [root@controller ~]# systemctl enable etcdCreated symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.[root@controller ~]# systemctl start etcd]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack</tag>
        <tag>ontroller</tag>
        <tag>mysql</tag>
        <tag>RabbitMQ</tag>
        <tag>Memcached</tag>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack queens实战(一)-准备工作]]></title>
    <url>%2F2018%2F06%2F15%2FOpenStack%2F2018-06-15-openstack_queens%E5%AE%9E%E6%88%98(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[节点信息简介 节点名 网卡IP 运行业务 备注 controller 192.168.100.10 10.10.10.10 20.20.20.10 MySQL(Marriad) RabbitMQ Keyston Memcached Etcd Horizon Neutron Nova 内存最少6G compute 192.168.100.20 10.10.10.20 20.20.20.20 Neutron Nova-Compute neutron 192.168.100.30 10.10.10.30 20.20.20.30 Neutron 因内存限制，该节点和Controller合并 cinder 192.168.100.40 10.10.10.40 20.20.20.40 Cinder-volume 需要额外增加虚拟磁盘 PS：本次实验使用Vmware创建了3台虚拟机，Controller、Compute、Cinder。Controller主机运行服务较多，4G内存不能满足需求，最少要分配6G。因台式主机内存限制，Neutron节点主机和Controller节点主机合并。 修改主机名项目实施初期创建一台虚拟主机，然后克隆拷贝。 [root@localhost ~]# hostnamectl --static set-hostname controller01[root@localhost ~]# hostnamectl status Static hostname: controllerTransient hostname: localhost.localdomain Icon name: computer-vm Chassis: vm Machine ID: 0343c8ebe09e405eaf7f760bb07a9e63 Boot ID: 4cebdb05127240e7a747e1bb8c5d29be Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-862.11.6.el7.x86_64 Architecture: x86-64[root@localhost ~]# reboot 域名解析配置vim /etc/hosts10.10.10.10 controller openstack-controller.com10.10.10.20 compute openstack-compute.com10.10.10.30 Network openstack-cinder.com10.10.10.40 cinder openstack-cinder.com 关闭防火墙和selinux[root@controller ~]# vim /etc/selinux/config # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.#SELINUX=enforcingSELINUX=disable# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted[root@localhost ~]# systemctl disable firewalld [root@localhost ~]# systemctl stop firewalld 配置国内源cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo wget -P /etc/yum.repos.d/ http://mirrors.aliyun.com/repo/epel-7.repo yum clean all yum makecache openstack服务安装、配置说明：无特殊说明，以下操作在所有节点上执行1.下载安装openstack软件仓库（queens版本）yum install centos-release-openstack-queens -y2.更新所有节点软件包yum upgrade3.安装openstack client端yum install python-openstackclient -y4.安装openstack-selinuxyum install openstack-selinux -y 安装NTP时钟服务controller节点安装软件包yum install chrony -y 编辑/etc/chrony.conf文件，配置时钟源同步服务端server time.windows.com iburst ##所有节点向controller节点同步时间allow 172.16.14.0/24 ##设置时间同步网段 设置NTP服务开机启动systemctl enable chronyd.servicesystemctl start chronyd.service 其他节点安装软件包yum install chrony -y配置所有节点指向controller同步时间vim /etc/chrony.confserver controlelr iburst 重启NTP服务systemctl restart chronyd.service 验证时钟同步服务#在controller节点执行[root@contorller yum.repos.d]# chronyc sources210 Number of sources = 1MS Name/IP address Stratum Poll Reach LastRx Last sample ===============================================================================^? time.windows.com 0 8 0 - +0ns[ +0ns] +/- 0ns#在其他节点执行[root@cinder ~]# chronyc sources210 Number of sources = 1MS Name/IP address Stratum Poll Reach LastRx Last sample ===============================================================================^? controller 0 9 0 - +0ns[ +0ns] +/- 0ns[root@compute ~]# chronyc sources210 Number of sources = 1MS Name/IP address Stratum Poll Reach LastRx Last sample ===============================================================================^? controller 0 9 0 - +0ns[ +0ns] +/- 0ns 本系列博客日志说明本次实验内容第一次编辑在2018年6月，9月底整理笔记成博客，因第一次日志记录不完整，本博客日志为后面补充，时间可能会有差别。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>opestack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-计划任务]]></title>
    <url>%2F2018%2F06%2F04%2FLinux%2F2018-06-04-Linux-%E8%AE%A1%E5%88%92%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[单次计划at 时间服务 atd 必须开启 [root@localhost ~]# at 13:25at&gt; echo `date` &gt; /root/date.txtat&gt; &lt;EOT&gt;job 2 at Mon Jun 4 13:25:00 2018[root@localhost ~]# at 13:32at&gt; echo `date` aaaaaaaaaaaaaaaa &gt; date.txt at&gt; &lt;EOT&gt;job 3 at Mon Jun 4 13:32:00 2018[root@localhost ~]# at now +10minat&gt; acho "now" &gt; now.txtat&gt; &lt;EOT&gt;job 4 at Mon Jun 4 13:39:00 2018[root@localhost ~]# at -l3 Mon Jun 4 13:32:00 2018 a root4 Mon Jun 4 13:39:00 2018 a root[root@localhost ~]# #删除任务[root@localhost ~]# atrm 4You have new mail in /var/spool/mail/root 周期性计划任务配置文件[root@localhost ~]# vim /etc/crontab* * * * *分 时 日 月 星# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed [root@localhost ~]# systemctl status crond● crond.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2018-06-01 11:57:19 CST; 3 days ago Main PID: 1225 (crond) Tasks: 1 CGroup: /system.slice/crond.service └─1225 /usr/sbin/crond -nJun 01 11:57:19 localhost.localdomain systemd[1]: Started Command Scheduler.Jun 01 11:57:19 localhost.localdomain systemd[1]: Starting Command Scheduler... 对于系统级别的计划任务，需要执行的命令和脚本都放在这里[root@localhost ~]# ll /etc/cron.cron.d/ cron.daily/ cron.deny cron.hourly/ cron.monthly/ cron.weekly/ 针对用户级别的计划任务：命令：crontab –e 创建一个计划任务crontab –l 显示crontab –r 删除计划任务crontab –e #写法分 时 日 月 星 谁做后面的事情 命令每个取值范围：分：0－59小时：0－23日：1－31月：1－12周：0－7 0/7 都是周日[root@localhost ~]# crontab -e0 21 * * * echo `date` &gt; /root/rm.txt 查看计划任务[root@localhost ~]# crontab -l0 21 * * * echo `date` &gt; /root/rm.txt 例：特殊写法#每月9,18,22号这几天的凌晨1点1分，执行一个备份脚本1 1 9,18,22 * * /root/backup.sh#每月9-22号这几天的凌晨1点1分，执行一个备份脚本1 1 9-22 * * /root/backup.sh#每5分钟，执行一次*/5 * * * * /root/backup.sh 使用root身份，给其它普通用户指定crontab：语法：crontab -u USERNAME -e/-l/-r[root@localhost ~]# crontab -u rm -e1 2 * * 5 poweroff[root@localhost ~]# crontab -u rm -l1 2 * * 5 poweroff删除[root@localhost ~]# crontab -r anacroncron用控制循环执行例行性工作。如果我要设定机器每早8点进行备份用服务。除非我机器保证在8点这个时间点不会关机，如果关机了，cron中的脚本，在下次开机将不会被执行。anacron并没有取代cron的意思，anacron用于，机器重启后，会侦测停机期间，有没有cron没有执行的计划任务，如果有，会立即，执行一下没有执行的任务。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>crontab</tag>
        <tag>at</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习-Number数字]]></title>
    <url>%2F2018%2F05%2F07%2FPython%2F2018-05-07-Python%E5%AD%A6%E4%B9%A0-Number%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[Number数字Python 数字数据类型用于存储数值。数据类型是不允许改变的,这就意味着如果改变数字数据类型的值，将重新分配内存空间。以下实例在变量赋值时 Number 对象将被创建：var1 = 1var2 = 10 您也可以使用del语句删除一些数字对象的引用。del语句的语法是：del var1[,var2[,var3[....,varN]]]]可以通过使用del语句删除单个或多个对象的引用，例如：del var del var_a, var_b Python 支持三种不同的数值类型： 整型(Int) 通常被称为是整型或整数，是正或负整数，不带小数点。Python3 整型是没有限制大小的，可以当作 Long 类型使用，所以 Python3 没有 Python2 的 Long 类型。 浮点型(float) 浮点型由整数部分与小数部分组成，浮点型也可以使用科学计数法表示（2.5e2 = 2.5 x 10 2 = 250） 复数( (complex)) 复数由实数部分和虚数部分构成，可以用a + bj,或者complex(a,b)表示， 复数的实部a和虚部b都是浮点型。 数字类型转换Python可将包含混合类型的表达式内部的数字转换成用于评估求值的常用类型。 有时需要从一个类型到另一个类型执行明确数字转换，以满足运算符或函数参数的要求。 int(x)将x转换为纯整数。 float(x)将x转换为浮点数。 complex(x)将x转换为具有实部x和虚部0的复数。 complex(x, y)将x和y转换为具有实部为x和虚部为y的复数。x和y是数字表达式。 数学函数 函数 返回值 ( 描述 ) abs(x) 返回数字的绝对值，如abs(-10) 返回 10 ceil(x) 返回数字的上入整数，如math.ceil(4.1) 返回 5 cmp(x, y) 如果 x &lt; y 返回 -1, 如果 x == y 返回 0, 如果 x &gt; y 返回 1。 Python 3 已废弃 。使用 使用 (x&gt;y)-(x&lt;y) 替换。 exp(x) 返回e的x次幂(ex),如math.exp(1) 返回2.718281828459045 fabs(x) 返回数字的绝对值，如math.fabs(-10) 返回10.0 floor(x) 返回数字的下舍整数，如math.floor(4.9)返回 4 log(x) 如math.log(math.e)返回1.0,math.log(100,10)返回2.0 log10(x) 返回以10为基数的x的对数，如math.log10(100)返回 2.0 max(x1, x2,…) 返回给定参数的最大值，参数可以为序列。 min(x1, x2,…) 返回给定参数的最小值，参数可以为序列。 modf(x) 返回x的整数部分与小数部分，两部分的数值符号与x相同，整数部分以浮点型表示。 pow(x, y) x**y 运算后的值。 round(x [,n]) 返回浮点数x的四舍五入值，如给出n值，则代表舍入到小数点后的位数。 sqrt(x) 返回数字x的平方根。 随机数函数 函数 描述 choice(seq) 从序列的元素中随机挑选一个元素，比如random.choice(range(10))，从0到9中随机挑选一个整数。 randrange ([start,] stop [,step]) 从指定范围内，按指定基数递增的集合中获取一个随机数，基数缺省值为1 random() 随机生成下一个实数，它在[0,1)范围内。 seed([x]) 改变随机数生成器的种子seed。如果你不了解其原理，你不必特别去设定seed，Python会帮你选择seed。 shuffle(lst) 将序列的所有元素随机排序 uniform(x, y) 随机生成下一个实数，它在[x,y]范围内。 三角函数 函数 描述 acos(x) 返回x的反余弦弧度值。 asin(x) 返回x的反正弦弧度值。 atan(x) 返回x的反正切弧度值。 atan2(y, x) 返回给定的 X 及 Y 坐标值的反正切值。 cos(x) 返回x的弧度的余弦值。 hypot(x, y) 返回欧几里德范数 sqrt(xx + yy)。 sin(x) 返回的x弧度的正弦值。 tan(x) 返回x弧度的正切值。 degrees(x) 将弧度转换为角度,如degrees(math.pi/2) ， 返回90.0 radians(x) 将角度转换为弧度 数学常量 常量 描述 pi 数学常量 pi（圆周率，一般以π来表示） e 数学常量 e，e即自然常数（自然常数）。]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python学习-运算符]]></title>
    <url>%2F2018%2F05%2F06%2FPython%2F2018-05-06-Python%E5%AD%A6%E4%B9%A0-%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[运算符Python语言支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 算术运算符 运算符 描述 实例 + 加法运算，将运算符两边的操作数增加 25 + 10 = 35 - 减法运算，将运算符左边的操作数减去右边的操作数 25 - 10 = 15 * 乘法运算，将运算符两边的操作数相乘 25 * 10 = 250 / 除法运算，用右操作数除左操作数 25 / 10 = 2.5 % 模运算，用右操作数除数左操作数并返回余数 25 % 10 = 5 ** 对运算符进行指数(幂)计算 25 ** 12 = 95367431640625 // 取整除 - 返回商的整数部分 25 // 10 = 2 代码实例#!/usr/local/bin/python3a = 25b = 10c = 0c = a + bprint("a + b = ", c)c = a - bprint("a - b = ", c)c = a * bprint("a * b = ", c)c = a / bprint("a / b = ", c)c = a % bprint("a % b = ", c)c = a ** bprint("a ** b = ", c)c = a // bprint("a // b = ", c) 输出结果a + b = 35a - b = 15a * b = 250a / b = 2.5a % b = 5a ** b = 95367431640625a // b = 2 比较运算符 运算符 描述 实例(a = 25，b = 10) == 等于 - 比较对象是否相等 (a == b) 返回 False。 != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True。 &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。 &lt; 小于 - 返回x是否小于y (a &lt; b) 返回 True。 &gt;= 大于等于 - 返回x是否大于等于y (a &gt;= b) 返回 False。 &lt;= 小于等于 - 返回x是否小于等于y (a &lt;= b) 返回 True。 代码实例a = 25b = 10if a == b: print("a等于b")else: print("a不等于b")if a != b: print("a不等于b")else: print("a等于b")if a &gt; b: print("a大于b")else: print("a不大于b")if a &lt; b: print("a小于b")else: print("a不小于b")if a &gt;= b: print("a大于等于b")else: print("a不大于等于b")if a &lt;= b: print("a小于等于b")else: print("a不小于等于b") 运行结果a不等于ba不等于ba大于ba不小于ba大于等于ba不小于等于b 赋值运算符 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a /*= 乘法赋值运算符 c *= a 等效于 c = c * a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c **= a 等效于 c = c ** a //= 取整除赋值运算符 c //= a 等效于 c = c // a 特别说明：Python不支持&quot;++&quot;，如i++。实例#!/usr/local/bin/python3a = 25b = 10c = 0c = a + bprint("a + b = c = ", c)c += aprint("c += a 即 c = c + a, c = ", c)c -= aprint("c -= a 即 c = c - a, c = ", c)c *= aprint("c *= a 即 c = c * a, c = ", c)c /= aprint("c /= a 即 c = c / a, c = ", c)c = 2c %= aprint("c %= a 即 c = c % a, c = ", c)c **= aprint("c **= a 即 c = c ** a, c = ", c)c //= aprint("c //= a 即 c = c // a, c = ", c) 结果a + b = c = 35c += a 即 c = c + a, c = 60c -= a 即 c = c - a, c = 35c *= a 即 c = c * a, c = 875c /= a 即 c = c / a, c = 35.0c %= a 即 c = c % a, c = 2c **= a 即 c = c ** a, c = 33554432c //= a 即 c = c // a, c = 1342177 逻辑运算符 运算符 描述 实例 and 如果两个操作数都为真，则条件成立。 (a and b)的结果为False or 如果两个操作数中的任何一个非零，则条件成为真。 (a or b)的结果为True not 用于反转操作数的逻辑状态。 not(a and b) 的结果为True 实例#!/usr/bin/python3a = 10b = 20if a and b: print("变量 a 和 b 都为 true")else: print("变量 a 和 b 有一个不为 true")if a or b: print("变量 a 和 b 都为 true，或其中一个变量为 true")else: print("变量 a 和 b 都不为 true")# 修改变量 a 的值a = 0if a and b: print("变量 a 和 b 都为 true")else: print("变量 a 和 b 有一个不为 true")if a or b: print("变量 a 和 b 都为 true，或其中一个变量为 true")else: print("变量 a 和 b 都不为 true")if not (a and b): print("变量 a 和 b 都为 false，或其中一个变量为 false")else: print("变量 a 和 b 都为 true") 结果变量 a 和 b 都为 true变量 a 和 b 都为 true，或其中一个变量为 true变量 a 和 b 有一个不为 true变量 a 和 b 都为 true，或其中一个变量为 true变量 a 和 b 都为 false，或其中一个变量为 false 位运算符 运算符 描述 实例(a=0011 1100,b=0000 1101) &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100 &#124; 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 (a&#124;b) 输出结果 61 ，二进制解释： 0011 1101 ^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1。~x 类似于 -x-1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。 &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由”&lt;&lt;”右边的数指定移动的位数，高位丢弃，低位补0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000 &gt;&gt; 右移动运算符：把”&gt;&gt;”左边的运算数的各二进位全部右移若干位，”&gt;&gt;”右边的数指定移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111 成员运算符 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。 not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 实例#!/usr/bin/python3a = 10b = 2list1 = [1, 2, 3, 4, 5]if a in list1: print("变量 a 在给定的列表list1中")else: print("变量 a 不在给定的列表list1中")if b not in list1: print("变量 b 不在给定的列表list1中")else: print("变量 b 在给定的列表list1中") 结果变量 a 不在给定的列表list1中变量 b 在给定的列表list1中 身份运算符 运算符 描述 实例 is is是判断两个标识符是不是引用自一个对象 x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False is not is not 是判断两个标识符是不是引用自不同对象 x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 实例#!/usr/bin/python3a = 20b = 20if a is b: print("a 和 b 有相同的标识")else: print("a 和 b 没有相同的标识")if id(a) == id(b): print("a 和 b 有相同的标识")else: print("a 和 b 没有相同的标识")# 修改变量 b 的值b = 30if a is b: print("a 和 b 有相同的标识")else: print("a 和 b 没有相同的标识")if a is not b: print("a 和 b 没有相同的标识")else: print("a 和 b 有相同的标识") 结果a 和 b 有相同的标识a 和 b 有相同的标识a 和 b 没有相同的标识a 和 b 没有相同的标识 运算符优先级 运算符 描述 ** 指数 (最高优先级) ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@) * / % // 乘，除，取模和取整除 + - 加法减法 &gt;&gt; &lt;&lt; 右移，左移运算符 &amp; 位 ‘AND’ ^ &#124; 位运算符 &lt;= &lt; &gt; &gt;= 比较运算符 &lt;&gt; == != 等于运算符 = %= /= //= -= += *= **= 赋值运算符 is is not 身份运算符 in not in 成员运算符 and or not 逻辑运算符]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python学习-变量类型]]></title>
    <url>%2F2018%2F05%2F05%2FPython%2F2018-05-05-Python%E5%AD%A6%E4%B9%A0-%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[标准变量类型Python3中有6种标准变量类型： Number（数字） String（字符串） List（列表） Tuple（元组） Set（集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组） 可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合） Number(数字)Python3中支持不同的数值之类型：int（整型），float（浮点型），complex（复数），bool（布尔型）。 int ：整数或长整型（Python只有int类型，Python2中有int和long） float：浮点数，浮点数也可以是科学符号，E或e表示10的幂 complex：复数是以a + bJ的形式，其中a和b是浮点，J(或j)表示-1的平方根(虚数)。数字的实部是a，虚部是b。 bool：布尔型，特殊的数值，只有0或1 &gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class 'int'&gt; &lt;class 'float'&gt; &lt;class 'bool'&gt; &lt;class 'complex'&gt; String(字符串)字符串用单引号 ‘ 或双引号 “ 括起来，同时使用反斜杠 \ 转义特殊字符。 可以使用片段运算符([]和[:])来获取字符串的子集(子字符串)，其索引从字符串开始处的索引0开始，并且以-1表示字符串中的最后一个字符。 加号(+)是字符串连接运算符，星号(*)是重复运算符。 List(列表)列表是Python复合数据类型中最多功能的。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 一个列表包含用逗号分隔并括在方括号([])中的项目。存储在列表中的值可以使用切片运算符([]和[])来访问，索引从列表开头的0开始，并且以-1表示列表中的最后一个项目。 加号(+)是列表连接运算符，星号(*)是重复运算符。 Tuple(元组)元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。 元组中的元素类型也可以不相同： Set(集合)集合（set）是由一个或数个形态各异的大小整体组成的，构成集合的事物或对象称作元素或是成员。可以使用大括号 { } 或者 set() 函数创建集合。 注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 Dictionary(字典)Python的字典是一种哈希表类型。由键值对组成。 字典是一种映射类型，字典用”{ }”标识，它是一个无序的键(key) : 值(value)对集合。 键(key)必须使用不可变类型。在同一个字典中，键(key)必须是唯一的。]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux-磁盘管理(三)]]></title>
    <url>%2F2018%2F04%2F20%2FLinux%2F2018-04-20-Linux-%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86(%E4%B8%89)%2F</url>
    <content type="text"><![CDATA[RDIDRAID介绍 RAID-0striping（条带模式），至少需要两块磁盘，做RAID分区的大小最好是相同的（可以充分发挥并发优势）;数据分散存储于不同的磁盘上，在读写的时候可以实现并发，所以相对其读写性能最好；但是没有容错功能，任何一个磁盘的损坏将损坏全部数据；磁盘利用率为100%。 RAID-1mirroring（镜像卷），至少需要两块硬盘，raid大小等于两个raid分区中最小的容量（最好将分区大小分为一样），数据有冗余，在存储时同时写入两块硬盘，实现了数据备份；磁盘利用率为50%，即2块100G的磁盘构成RAID1只能提供100G的可用空间。 RAID-5需要三块或以上硬盘，可以提供热备盘实现故障的恢复；只损坏一块，没有问题。但如果同时损坏两块磁盘，则数据将都会损坏。 空间利用率： (n-1)/n是用相对简单的异或逻辑运算（相同为0，相异为1）A值 B值 Xor结果0 0 01 0 10 1 11 1 0 RAID10Raid0 + raid1 管理软raid工具mdadm mdadm 参数命令常用参数如下： 热备份盘(hot spare or hot standby driver)为了加强容错的功能以及使系统在磁盘故障的情况下能迅速的重建数据,以维持系统的性能,一般的磁盘阵列系统都可使用热备份功能。 Chunk （块）raid存储数据时每个数据段的大小。 4K，64K若chunk过大，可能一块磁盘上的带区空间就可以满足大部分的I/O操作的数据的读写只局限与一块硬盘上，这便不能充分发挥Raid并发的优势；如果chunk设置过小，任何很小的I/O指令都可能引发大量的读写操作，不能良好的发挥并发性能，占用过多的控制器总线带宽，也影响了阵列的整体性能。所以，在创建带区时，我们应该根据实际应用的需要，合理的选择带区的大小 Raid01、创建RAID02、导出阵列配置文件3、格式化并挂载到指定目录4、修改/etc/fstab永久挂载 #环境：添加一个sdb硬盘，分两个1G的主分区。sdb1和sdb2[root@izuf6fmbh00axmyjllc0x0z ~]# fdisk /dev/sdb[root@izuf6fmbh00axmyjllc0x0z ~]# ll /dev/sdb*brw-rw---- 1 root disk 8, 16 Feb 25 09:10 /dev/sdbbrw-rw---- 1 root disk 8, 17 Feb 25 09:10 /dev/sdb1brw-rw---- 1 root disk 8, 18 Feb 25 09:10 /dev/sdb2 [root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -C -v /dev/md0 -l 0 -n 2 /dev/sdb&#123;1,2&#125;mdadm: chunk size defaults to 512Kmdadm: Defaulting to version 1.2 metadatamdadm: array /dev/md0 started.#查看阵列信息[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -DsARRAY /dev/md0 metadata=1.2 name=izuf6fmbh00axmyjllc0x0z :0 UUID=c02009dd:fc28f287:fafca47a:49b2bd81#生成配置文件[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ds &gt; /etc/mdadm.conf[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -D /dev/md0/dev/md0: Version : 1.2 Creation Time : Thu Feb 25 09:14:14 2016 Raid Level : raid0 Array Size : 2095104 (2046.34 MiB 2145.39 MB) Raid Devices : 2 Total Devices : 2 Persistence : Superblock is persistent Update Time : Thu Feb 25 09:14:14 2016 State : clean Active Devices : 2Working Devices : 2 Failed Devices : 0 Spare Devices : 0 Chunk Size : 512K Name : izuf6fmbh00axmyjllc0x0z:0 (local to host izuf6fmbh00axmyjllc0x0z) UUID : c02009dd:fc28f287:fafca47a:49b2bd81 Events : 0 Number Major Minor RaidDevice State 0 8 17 0 active sync /dev/sdb1 1 8 18 1 active sync /dev/sdb2#对创建的RAID0创建分区[root@izuf6fmbh00axmyjllc0x0z ~]# fdisk /dev/md0[root@izuf6fmbh00axmyjllc0x0z ~]# ll /dev/md0*brw-rw---- 1 root disk 9, 0 Feb 25 09:19 /dev/md0brw-rw---- 1 root disk 259, 1 Feb 25 09:19 /dev/md0p1#格式化分区并挂载[root@izuf6fmbh00axmyjllc0x0z ~]# mkfs.xfs /dev/md0p1[root@izuf6fmbh00axmyjllc0x0z ~]# mkdir /raid0[root@izuf6fmbh00axmyjllc0x0z ~]# mount /dev/md0p1 /raid0/#永久挂载[root@izuf6fmbh00axmyjllc0x0z ~]# blkid | grep md0p1/dev/md0p1: UUID="a25acfcc-9e33-474a-98b8-a0c07fee063e" TYPE="xfs"[root@izuf6fmbh00axmyjllc0x0z ~]# echo "UUID=a25acfcc-9e33-474a-98b8-a0c07fee063e /raid0 xfs defaults 0 0" &gt;&gt; /etc/fstab Raid1[root@izuf6fmbh00axmyjllc0x0z ~]# fdisk /dev/sdc[root@izuf6fmbh00axmyjllc0x0z ~]# ll /dev/sdc*brw-rw---- 1 root disk 8, 32 Feb 26 07:35 /dev/sdcbrw-rw---- 1 root disk 8, 33 Feb 26 07:35 /dev/sdc1brw-rw---- 1 root disk 8, 34 Feb 26 07:35 /dev/sdc2brw-rw---- 1 root disk 8, 35 Feb 26 07:35 /dev/sdc3 #创建RAID1[root@izuf6fmbh00axmyjllc0x0z dev]# mdadm -C -v /dev/md1 -l 1 -n 2 -x 1 /dev/vdb&#123;1,2,3&#125;mdadm: /dev/vdb1 appears to be part of a raid array: level=raid0 devices=2 ctime=Fri Jun 1 16:56:46 2018mdadm: Note: this array has metadata at the start and may not be suitable as a boot device. If you plan to store '/boot' on this device please ensure that your boot-loader understands md/v1.x metadata, or use --metadata=0.90mdadm: /dev/vdb2 appears to be part of a raid array: level=raid0 devices=2 ctime=Fri Jun 1 16:56:46 2018mdadm: size set to 3142656KContinue creating array? ymdadm: Defaulting to version 1.2 metadatamdadm: array /dev/md1 started. #查看阵列信息[root@izuf6fmbh00axmyjllc0x0z ~]# cat /proc/mdstatPersonalities : [raid0] [raid1]md1 : active raid1 vdb3[2](S) vdb2[1] vdb1[0] 3142656 blocks super 1.2 [2/2] [UU]unused devices: &lt;none&gt; #生成配置文件[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ds &gt; /etc/mdadm.conf[root@izuf6fmbh00axmyjllc0x0z ~]# cat !$cat /etc/mdadm.confARRAY /dev/md1 metadata=1.2 spares=1 name=izuf6fmbh00axmyjllc0x0z:1 UUID=0c3af470:a80661c0:8e7d8ea8:aa6c38e5 #使用RAID1 创建分区、格式化、挂载[root@izuf6fmbh00axmyjllc0x0z ~]# fdisk /dev/md1[root@izuf6fmbh00axmyjllc0x0z ~]# mkfs.xfs /dev/md1p1meta-data=/dev/md1p1 isize=512 agcount=4, agsize=196352 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0data = bsize=4096 blocks=785408, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0[root@izuf6fmbh00axmyjllc0x0z ~]# mkdir /raid1[root@izuf6fmbh00axmyjllc0x0z ~]# mount /dev/md1p1 /raid1[root@izuf6fmbh00axmyjllc0x0z ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 6.7G 31G 18% /devtmpfs 486M 0 486M 0% /devtmpfs 496M 0 496M 0% /dev/shmtmpfs 496M 552K 496M 1% /runtmpfs 496M 0 496M 0% /sys/fs/cgrouptmpfs 100M 0 100M 0% /run/user/5005/dev/md1p1 3.0G 33M 3.0G 2% /raid1[root@izuf6fmbh00axmyjllc0x0z ~]# cat /proc/mdstatPersonalities : [raid0] [raid1]md1 : active raid1 vdb3[2](S) vdb2[1] vdb1[0] 3142656 blocks super 1.2 [2/2] [UU]unused devices: &lt;none&gt; 部分信息介绍#md1:表示此阵列的设备名#active：表示此阵列正常读写#sdc3[2][S]：表示是这个阵列第3个设备且是备用盘，sdc2[1]是此阵列第2个设备，sdc1[0]是此阵列第1个设备# 11047552 blocks：表示此阵列的大小，以块为单位（1G）#[2/2]：表示阵列中有2个磁盘，并且2个都在正常运行[root@izuf6fmbh00axmyjllc0x0z ~]# cat /etc/mdadm.confARRAY /dev/md1 metadata=1.2 spares=1 name=izuf6fmbh00axmyjllc0x0z:1 UUID=9b5e920c:a637e13a:9bfc64ac:74998dc0#spares=1表示：有热备盘1个 模拟磁盘故障，移除故障盘 [root@izuf6fmbh00axmyjllc0x0z ~]# watch -n 1 cat /proc/mdstat[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -f /dev/md1 /dev/vdb1mdadm: set /dev/vdb1 faulty in /dev/md1 #移动损坏的设备：[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -r /dev/md1 /dev/vdb1mdadm: hot removed /dev/vdb1 from /dev/md1[root@xuegod61 ~]# cat /proc/mdstatPersonalities : [raid0] [raid1]md1 : active raid1 vdb3[2] vdb2[1] 3142656 blocks super 1.2 [2/2] [UU]unused devices: &lt;none&gt; #重新生成配置文件[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ds &gt; /etc/mdadm.conf[root@izuf6fmbh00axmyjllc0x0z ~]# cat !$cat /etc/mdadm.confARRAY /dev/md1 metadata=1.2 name=izuf6fmbh00axmyjllc0x0z:1 UUID=0c3af470:a80661c0:8e7d8ea8:aa6c38e5 Raid5#准备测试磁盘[root@izuf6fmbh00axmyjllc0x0z ~]# ll /dev/vdb*brw-rw---- 1 root disk 253, 16 Jun 2 14:05 /dev/vdbbrw-rw---- 1 root disk 253, 17 Jun 2 14:05 /dev/vdb1brw-rw---- 1 root disk 253, 18 Jun 2 14:05 /dev/vdb2brw-rw---- 1 root disk 253, 19 Jun 2 14:05 /dev/vdb3brw-rw---- 1 root disk 253, 21 Jun 2 14:05 /dev/vdb5brw-rw---- 1 root disk 253, 22 Jun 2 14:05 /dev/vdb6brw-rw---- 1 root disk 253, 23 Jun 2 14:05 /dev/vdb7brw-rw---- 1 root disk 253, 24 Jun 2 14:05 /dev/vdb8brw-rw---- 1 root disk 253, 25 Jun 2 14:05 /dev/vdb9#创建Raid5[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -C -v /dev/md5 -l 5 -n 3 -c 32 -x 1 /dev/vdb&#123;1,2,5,6&#125;mdadm: layout defaults to left-symmetricmdadm: layout defaults to left-symmetricmdadm: /dev/vdb1 appears to be part of a raid array: level=raid5 devices=3 ctime=Sat Jun 2 13:56:47 2018mdadm: size set to 2094080KContinue creating array? ymdadm: Defaulting to version 1.2 metadatamdadm: array /dev/md5 started.#创建配置文件[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ds &gt; /etc/mdadm.conf[root@izuf6fmbh00axmyjllc0x0z ~]# cat !$cat /etc/mdadm.confARRAY /dev/md5 metadata=1.2 spares=2 name=izuf6fmbh00axmyjllc0x0z:5 UUID=c9c1b14c:317965ac:30d712d7:bd9b6aaa#PS：停止raid5（停止之前必须生成配置文件）#停止raid5[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -S /dev/md5mdadm: stopped /dev/md5[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ds#激活Raid[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Asmdadm: /dev/md5 has been started with 3 drives and 1 spare.#添加一个新的设备到RAID[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -a /dev/md5 /dev/vdb7mdadm: added /dev/vdb7[root@izuf6fmbh00axmyjllc0x0z ~]# cat /proc/mdstatPersonalities : [raid0] [raid1] [raid6] [raid5] [raid4]md5 : active raid5 vdb7[5](S) vdb1[0] vdb6[3](S) vdb5[4] vdb2[1] 4188160 blocks super 1.2 level 5, 32k chunk, algorithm 2 [3/3] [UUU]unused devices: &lt;none&gt;#扩容#PS：扩容后先生成配置文件，然后在查看相关信息。[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -G /dev/md5 -n 4[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -DsARRAY /dev/md5 metadata=1.2 spares=1 name=izuf6fmbh00axmyjllc0x0z:5 UUID=c9c1b14c:317965ac:30d712d7:bd9b6aaa[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ds &gt; /etc/mdadm.conf[root@izuf6fmbh00axmyjllc0x0z ~]# cat /proc/mdstatPersonalities : [raid0] [raid1] [raid6] [raid5] [raid4]md5 : active raid5 vdb7[5] vdb1[0] vdb6[3](S) vdb5[4] vdb2[1] 6282240 blocks super 1.2 level 5, 32k chunk, algorithm 2 [4/4] [UUUU]unused devices: &lt;none&gt; Raid10建立RAID 1+0双层架构的方法方法，先创建raid1，再使用创建的raid1设备创建raid0[root@izuf6fmbh00axmyjllc0x0z ~]# fdisk /dev/sdf[root@izuf6fmbh00axmyjllc0x0z ~]# ll /dev/sdf*brw-rw---- 1 root disk 8, 80 Feb 26 08:39 /dev/sdfbrw-rw---- 1 root disk 8, 81 Feb 26 08:39 /dev/sdf1brw-rw---- 1 root disk 8, 82 Feb 26 08:39 /dev/sdf2brw-rw---- 1 root disk 8, 83 Feb 26 08:39 /dev/sdf3brw-rw---- 1 root disk 8, 84 Feb 26 08:39 /dev/sdf4brw-rw---- 1 root disk 8, 85 Feb 26 08:39 /dev/sdf5 #创建2个底层的raid1[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -C -v /dev/md11 -l 1 -n 2 /dev/sdf&#123;1,2&#125;[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -C -v /dev/md12 -l 1 -n 2 /dev/sdf&#123;3,5&#125; #创建上层的raid0[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -C -v /dev/md10 -l 0 -n 2 /dev/md1&#123;1,2&#125;mdadm: chunk size defaults to 512Kmdadm: Defaulting to version 1.2 metadatamdadm: array /dev/md10 started. #生成配置文件[root@izuf6fmbh00axmyjllc0x0z ~]# cat !$cat /etc/mdadm.confARRAY /dev/md1 metadata=1.2 name=izuf6fmbh00axmyjllc0x0z :1 UUID=9b5e920c:a637e13a:9bfc64ac:74998dc0ARRAY /dev/md5 metadata=1.2 spares=1 name=izuf6fmbh00axmyjllc0x0z :5 UUID=1ac1f8c8:d4829dbb:a47fa5e2:414231f1ARRAY /dev/md11 metadata=1.2 name=izuf6fmbh00axmyjllc0x0z :11 UUID=c80e5768:eca7d5fb:6a576707:06884d89ARRAY /dev/md12 metadata=1.2 name=izuf6fmbh00axmyjllc0x0z :12 UUID=46da12ab:4b6a9a71:16d2715b:18e52724ARRAY /dev/md10 metadata=1.2 name=izuf6fmbh00axmyjllc0x0z :10 UUID=632a0efc:66e6847c:615ee844:a04ab1f3 #关闭所有的RAID#卸载raid的挂载[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ss #全部激活[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -As mdadm删除软raid的方法 #创建一个raid1[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -C /dev/md3 -l 1 -n 2 /dev/sdb&#123;1,2&#125; #开始删除raid[root@izuf6fmbh00axmyjllc0x0z ~]# umount /dev/md3p1[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm -Ss #删除配置文件[root@izuf6fmbh00axmyjllc0x0z ~]# rm -rf /etc/mdadm.conf #清除物理磁盘中的raid标识[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm --misc --zero-superblock /dev/sdb1[root@izuf6fmbh00axmyjllc0x0z ~]# mdadm --misc --zero-superblock /dev/sdb2 #参数说明：mdadm --misc options... devices report on or modify various md related devices. #报告或修改各种MD相关的设备--zero-superblock : erase the MD superblock from a device. #擦除设备中的MD超级块。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>RAID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-磁盘管理(二)]]></title>
    <url>%2F2018%2F04%2F19%2FLinux%2F2018-04-19-Linux-%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[LVM创建PE[root@izuf6fmbh00axmyjllc0x0z ~]# pvcreate /dev/vdb&#123;1,3&#125;WARNING: ext4 signature detected on /dev/vdb1 at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/vdb1.WARNING: ext4 signature detected on /dev/vdb3 at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/vdb3. Physical volume "/dev/vdb1" successfully created. Physical volume "/dev/vdb3" successfully created.[root@izuf6fmbh00axmyjllc0x0z ~]# pvcreate /dev/vdb2WARNING: ext4 signature detected on /dev/vdb2 at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/vdb2. Physical volume "/dev/vdb2" successfully created.[root@izuf6fmbh00axmyjllc0x0z ~]# pvs PV VG Fmt Attr PSize PFree /dev/vdb1 lvm2 --- 10.00g 10.00g /dev/vdb2 lvm2 --- 5.00g 5.00g /dev/vdb3 lvm2 --- &lt;5.00g &lt;5.00g 创建VGvgcreate -s 16M VG1 /dev/vdb1 /dev/vdb2 -s 16M 指定PE大小[root@izuf6fmbh00axmyjllc0x0z ~]# vgcreate VG1 /dev/vdb1 /dev/vdb2 Volume group "VG1" successfully created 创建LV[root@izuf6fmbh00axmyjllc0x0z ~]# lvcreate -n LV1 -L 12G VG1 Logical volume "LV1" created.#格式化LV[root@izuf6fmbh00axmyjllc0x0z ~]# mkfs.ext4 /dev/VG1/LV1mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks786432 inodes, 3145728 blocks157286 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=215167795296 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208Allocating group tables: doneWriting inode tables: doneCreating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done 挂载[root@izuf6fmbh00axmyjllc0x0z /]# mount /dev/VG1/LV1 /opt[root@izuf6fmbh00axmyjllc0x0z /]# ls !$ls /optlost+found 扩容#LV扩容[root@izuf6fmbh00axmyjllc0x0z /]# lvextend -L +2G /dev/VG1/LV1 Size of logical volume VG1/LV1 changed from 12.00 GiB (3072 extents) to 14.00 GiB (3584 extents). Logical volume VG1/LV1 successfully resized.[root@izuf6fmbh00axmyjllc0x0z /]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 6.6G 31G 18% /devtmpfs 486M 0 486M 0% /devtmpfs 496M 0 496M 0% /dev/shmtmpfs 496M 736K 496M 1% /runtmpfs 496M 0 496M 0% /sys/fs/cgrouptmpfs 100M 0 100M 0% /run/user/5005/dev/mapper/VG1-LV1 12G 41M 12G 1% /opt[root@izuf6fmbh00axmyjllc0x0z /]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert LV1 VG1 -wi-ao---- 14.00g[root@izuf6fmbh00axmyjllc0x0z /]# resize2fs /dev/VG1/LV1resize2fs 1.42.9 (28-Dec-2013)Filesystem at /dev/VG1/LV1 is mounted on /opt; on-line resizing requiredold_desc_blocks = 2, new_desc_blocks = 2The filesystem on /dev/VG1/LV1 is now 3670016 blocks long.[root@izuf6fmbh00axmyjllc0x0z /]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 6.6G 31G 18% /devtmpfs 486M 0 486M 0% /devtmpfs 496M 0 496M 0% /dev/shmtmpfs 496M 736K 496M 1% /runtmpfs 496M 0 496M 0% /sys/fs/cgrouptmpfs 100M 0 100M 0% /run/user/5005/dev/mapper/VG1-LV1 14G 41M 13G 1% /opt VG扩展[root@izuf6fmbh00axmyjllc0x0z /]# vgextend VG1 /dev/vdb3 Volume group "VG1" successfully extended[root@izuf6fmbh00axmyjllc0x0z /]# pvs PV VG Fmt Attr PSize PFree /dev/vdb1 VG1 lvm2 a-- &lt;10.00g 0 /dev/vdb2 VG1 lvm2 a-- &lt;5.00g 1016.00m /dev/vdb3 VG1 lvm2 a-- &lt;5.00g &lt;5.00g[root@izuf6fmbh00axmyjllc0x0z /]# vgs VG #PV #LV #SN Attr VSize VFree VG1 3 1 0 wz--n- &lt;19.99g &lt;5.99g LV缩减#先检查磁盘[root@izuf6fmbh00axmyjllc0x0z zhangq]# umount /opt/[root@izuf6fmbh00axmyjllc0x0z zhangq]# e2fsck -f /dev/VG1/LV1e2fsck 1.42.9 (28-Dec-2013)第一步: 检查inode,块,和大小第二步: 检查目录结构第3步: 检查目录连接性Pass 4: Checking reference counts第5步: 检查簇概要信息/dev/VG1/LV1: 11/917504 files (0.0% non-contiguous), 100612/3670016 blocks[root@izuf6fmbh00axmyjllc0x0z zhangq]# resize2fs /dev/VG1/LV1 10Gresize2fs 1.42.9 (28-Dec-2013)Resizing the filesystem on /dev/VG1/LV1 to 2621440 (4k) blocks.The filesystem on /dev/VG1/LV1 is now 2621440 blocks long.[root@izuf6fmbh00axmyjllc0x0z zhangq]# lvreduce -L 10G /dev/VG1/LV1 WARNING: Reducing active logical volume to 10.00 GiB. THIS MAY DESTROY YOUR DATA (filesystem etc.)Do you really want to reduce VG1/LV1? [y/n]: y Size of logical volume VG1/LV1 changed from 14.00 GiB (3584 extents) to 10.00 GiB (2560 extents). Logical volume VG1/LV1 successfully resized. VG缩减[root@izuf6fmbh00axmyjllc0x0z zhangq]# pvs PV VG Fmt Attr PSize PFree /dev/vdb1 VG1 lvm2 a-- &lt;10.00g 0 /dev/vdb2 VG1 lvm2 a-- &lt;5.00g 4.99g /dev/vdb3 VG1 lvm2 a-- &lt;5.00g &lt;5.00g[root@izuf6fmbh00axmyjllc0x0z zhangq]# vgreduce VG1 /dev/vdb3 Removed "/dev/vdb3" from volume group "VG1" LVM删除[root@izuf6fmbh00axmyjllc0x0z zhangq]# lvremove /dev/VG1/LV1Do you really want to remove active logical volume VG1/LV1? [y/n]: y Logical volume "LV1" successfully removed[root@izuf6fmbh00axmyjllc0x0z zhangq]# lvs[root@izuf6fmbh00axmyjllc0x0z zhangq]# vgremove VG1 Volume group "VG1" successfully removed[root@izuf6fmbh00axmyjllc0x0z zhangq]# pvs PV VG Fmt Attr PSize PFree /dev/vdb1 lvm2 --- 10.00g 10.00g /dev/vdb2 lvm2 --- 5.00g 5.00g /dev/vdb3 lvm2 --- &lt;5.00g &lt;5.00g[root@izuf6fmbh00axmyjllc0x0z zhangq]# pvremove /dev/vdb3 Labels on physical volume "/dev/vdb3" successfully wiped.[root@izuf6fmbh00axmyjllc0x0z zhangq]# pvremove /dev/vdb2 Labels on physical volume "/dev/vdb2" successfully wiped.[root@izuf6fmbh00axmyjllc0x0z zhangq]# pvremove /dev/vdb1 Labels on physical volume "/dev/vdb1" successfully wiped.[root@izuf6fmbh00axmyjllc0x0z zhangq]# pvs 快照创建一个名字为lv_NAME_bak ，大小为200M的快照lvcreate -s -n lv_NAME_bak -L 200M /dev/VG1/LV1]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-磁盘管理(一)]]></title>
    <url>%2F2018%2F04%2F18%2FLinux%2F2018-04-18-Linux-%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[DFdf 查看已挂载磁盘的总容量、使用容量、剩余容量等，可以不加任何参数，默认是按k为单位显示。 -a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统；-k ：以 KBytes 的容量显示各文件系统；-m ：以 MBytes 的容量显示各文件系统；-h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；-H ：以 M=1000K 取代 M=1024K 的进位方式；-T ：显示文件系统类型, 连同该 partition 的 filesystem 名称 (例如 ext3) 也列出；-i ：不用硬盘容量，而以 inode 的数量来显示 DULinux du命令也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的。 -a ：列出所有的文件与目录容量，因为默认仅统计目录底下的文件量而已。-h ：以人们较易读的容量格式 (G/M) 显示；-s ：列出总量而已，而不列出每个各别的目录占用容量；-S ：不包括子目录下的总计，与 -s 有点差别。-k ：以 KBytes 列出容量显示；-m ：以 MBytes 列出容量显示； FDISKLinux fdisk是一个创建和维护分区表的程序。只能划分2T以内的硬盘。 Command (m for help): mCommand action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition g create a new empty GPT partition table G create an IRIX (SGI) partition table l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only)[root@izuf6fmbh00axmyjllc0x0z ~]# ls /dev/vd*/dev/vda /dev/vda1 /dev/vdb[root@izuf6fmbh00axmyjllc0x0z ~]# fdisk /dev/vdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x48f5c900.Command (m for help): nPartition type: p primary (0 primary, 0 extended, 4 free) e extendedSelect (default p): pPartition number (1-4, default 1): First sector (2048-41943039, default 2048): Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-41943039, default 41943039): Using default value 41943039Partition 1 of type Linux and of size 20 GiB is setCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.[root@izuf6fmbh00axmyjllc0x0z ~]# ls /dev/vd*/dev/vda /dev/vda1 /dev/vdb /dev/vdb1[root@izuf6fmbh00axmyjllc0x0z ~]# PARTED当磁盘空间大于2T时，fdsik指令就不能满足要求。有两个方法来解决这个问题：其一是通过卷管理来实现，其二就是通过Parted工具来实现对2T磁盘进行分区操作。下面介绍parted。 #查看所有的磁盘状态[root@izuf6fmbh00axmyjllc0x0z ~]#parted -l 通过parted 工具创建新的分区[root@izuf6fmbh00axmyjllc0x0z ~]# parted /dev/sdbGNU Parted 3.1Using /dev/sdbWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) p #查看磁盘分区状态Error: /dev/sdb: unrecognised disk label (parted) mklabel #指定创建分区表类型为GPTNew disk label type? gpt (parted) mkpart #创建分区 Partition name? []? mydisk1 #指定分区文件系统类型，默认就可以，因为后期对分区进行格式化的时候，同样可以指定File system type? [ext2]? #指定分区起始位置 Start? 1#指定分区结束位置End? 100M #查看磁盘分区状态(parted) p Number Start End Size File system Name Flags 1 1049kB 99.6MB 98.6MB mydisk1(parted) mkpartPartition name? []? 2File system type? [ext2]?(parted) quitInformation: You may need to update /etc/fstab.End? 200M(parted) pModel: VMware, VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 1049kB 99.6MB 98.6MB mydisk1 2 99.6MB 200MB 101MB 2 (parted) quitInformation: You may need to update /etc/fstab. #删除GPT分区(parted) rm 2 #扩展swap#首先创建一个新的分区/dev/sda3 1G#格式化sda3分区为swap格式[root@izuf6fmbh00axmyjllc0x0z ~]# mkswap /dev/sda3Setting up swapspace version 1, size = 1048572 KiBno label, UUID=9a3f0877-4167-441e-95ba-5212da679562[root@izuf6fmbh00axmyjllc0x0z ~]# free -m total used free shared buff/cache availableMem: 3937 336 3290 9 310 3373Swap: 2047 0 2047 #扩展swap分区[root@izuf6fmbh00axmyjllc0x0z ~]# swapon /dev/sda3[root@izuf6fmbh00axmyjllc0x0z ~]# free -m total used free shared buff/cache availableMem: 3937 336 3289 9 310 3372Swap: 3071 0 3071 #开机自动挂载[root@izuf6fmbh00axmyjllc0x0z ~]# vim /etc/fstab/dev/sda3 swap swap defaults 0 0#注：只有重启才能生效，mount –a 无法自动扩展swap分区的。#关闭swap分区[root@izuf6fmbh00axmyjllc0x0z ~]# swapoff /dev/sda3 MOUNT mount挂载磁盘 mount [-t 文件系统] [-L Label名] [-o 额外选项] [-n] 装置文件名 挂载点[root@izuf6fmbh00axmyjllc0x0z ~]# mount /dev/vdb1 /mnt/vdb1mount: /dev/vdb1 is write-protected, mounting read-onlymount: unknown filesystem type '(null)'[root@izuf6fmbh00axmyjllc0x0z ~]# mkfs.ext4 /dev/vdb1mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks1310720 inodes, 5242624 blocks262131 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=2153775104160 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done [root@izuf6fmbh00axmyjllc0x0z ~]# mount /dev/vdb1 /mnt/vdb1 UMOUNTumount [-fn] 装置文件名或挂载点 自动挂载etc/fstab 要挂载的分区 挂载点 文件系统 挂载选项 是否备份 开始是否检查文件系统/dev/sr0 /mnt/cdrom iso9660 defaults 0 0 #挂载光盘[root@localhost zhangq]# mkdir /mnt/cdrom[root@localhost zhangq]# ls /mnt/cdrom/[root@localhost /]# mount /dev/sr0 /mnt/cdrom/mount: /dev/sr0 is write-protected, mounting read-only[root@localhost /]# ls /mnt/cdrom/CentOS_BuildTag GPL LiveOS RPM-GPG-KEY-CentOS-7EFI images Packages RPM-GPG-KEY-CentOS-Testing-7EULA isolinux repodata TRANS.TBL[root@localhost /]# vim /etc/fstab /dev/sr0 /mnt/cdrom iso9660 defaults 0 0[root@localhost /]# vim /etc/yum.repos.d/CentOS-Media.repo[c7-media] baseurl=file:///mnt/cdrom/ #在这里加一行我们刚才光盘挂载的路径 gpgcheck=1 enabled=1]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>df</tag>
        <tag>du</tag>
        <tag>fdisk</tag>
        <tag>parted</tag>
        <tag>mount</tag>
        <tag>umount</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-软件包管理]]></title>
    <url>%2F2018%2F04%2F15%2FLinux%2F2018-04-15-Linux-%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux-软件包管理 本博客中Linux系统是CentOS7。CentOS中软件安装有三种方式： rpm软件包 yum指令 源码编译安装 特点： rpm/yum：方便，软件版本低。稳定性好、管理方便。性能稍差。 源码手动：麻烦，软件版本新。稳定性稍差、管理稍差。性能好。 RPM 挂载光盘mount /dev/cdrom /mnt/ 查看包个数ls /mnt/Packages/ | wc -l 安装包rpm -ivh /mnt/Packages/zsh-4.3.10-4.1.el6.x86_64.rpm #参数说明：-i, --install install package(s)-v, --verbose provide more detailed output-h, --hash print hash marks as package installs (good with -v)#安装时需要注意依赖关系：找依赖库：1、从头开始查找2、过滤查找 *DB*3、上rpm包相关网站查找 rpmfind.net rpm.pbone.net www.rpmseek.com/index.html #强制安装： --nodeps rpm查询： rpm -qa zsh #查看是否安装成功rpm -ql zsh #查询安装后的文件和目录rpm -qf /bin/zsh #查询命令安装包是哪个#cat /etc/shellszsh UNIX 系统下的一种shell，rpm -pql #提前查询安装包产生哪些文件和目录rpm -qpl #查看rpm包内容 rpm -Uvh #升级安装包 rpm -e #卸载 rpm -e --nodeps #强制卸载 rpm -qpi #查看一个包的作用 NOKEY #导入rpmPGP cat /etc/yum.repos.d/rhel-source.repo rpm --import etc/pki/rpm-gpg/RPM-GPG-KEY YUM 配置yum源 cd /etc/yum.repos.d/#用于区别各个不同的yun软件包库，必须有一个独一无二的名称[epel]#对yum软件包库的描述name=Extra Packages for Enterprise Linux 7 - $basearch# 1表示为可用enabled=1 # failovermethod=priority#安装软件所在目录baseurl=http://mirrors.cloud.aliyuncs.com/epel/7/$basearch#0：不使用公钥校验rpm正确性 1：使用公钥校验rpm正确性gpgcheck=0#GPG校验公钥路径gpgkey=http://mirrors.cloud.aliyuncs.com/epel/RPM-GPG-KEY-EPEL-7 yum源安装yum clear all #清空本地缓存yum list yum list | grep #过滤查找yum search #查找yum install -y #直接安装yum grouplistyum groupinstall &quot;Development tolls&quot; #安装一组软件包yum remove #删除软件包 编译源码安装软件前提：系统必须安装开发工具、开发库 安装步骤 1、获得源码包2、解压 tar -jxvf 3、配置，检测安装环境 ./configure ./configure --prefix=/urs/local/… #指定安装路径4、编译 make5、安装 make install 删除 make uninstall 指定路径的软件可以直接删除软件所在文件夹即可]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>rpm</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-文件归档]]></title>
    <url>%2F2018%2F04%2F12%2FLinux%2F2018-04-12-Linux-%E6%96%87%E4%BB%B6%E5%BD%92%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[tar功能：Linux中常用的压缩和解压命令。参数：有必选项和可选项。必选项：-c 创建压缩文件-r 追加文件在档案文件的未尾-t 查看文件-u 更新文件-x 解压 可选项：-b 该选项是为磁带机设定的，其后跟一数字，用来说明区块的大小，系统预设值为20（20×512 bytes）。-f 指定档案文件名或设备名，这个选项通常是必选的。-k 解压文档时部覆盖已有文件-m 在还原文件时，把所有文件的修改时间设定为现在。-M 创建多卷的档案文件-v 显示详细执行过程-w 每一步都要求确认。-z 用gzip来压缩/解压缩文件-j 用bz2来压缩/解压文件 实例： 归档+压缩 gztar -zcvf a.tar.gz /boottar -zxvf a.tar.gz -C /root/test 归档压缩 bz2tar -jcvf a.tar.bz2 /boot/grub/tar -jxvf a.tar.bz2 /boot/grub/Tar例：把两个目录或目标+文件打包成一个软件包[root@localhost ~]# tar cvf ss.tar /boot/ /etc/passwd不解包，查看tar中的内容：[root@localhost ~]# tar tvf grub2.tar 解包：[root@localhost ~]# tar xvf grub2.tar 指定解压路径：[root@localhost ~]# tar xvf grub2.tar -C /opt/ 对比文件的大小[root@localhost ~]# du -sh /boot/grub2/8.1M /boot/grub2/[root@localhost ~]# ll -h grub2.tar-rw-r--r-- 1 root root 7.7M Feb 17 07:40 grub2.tar zip功能：对文档进行压缩，后缀为“.zip”。 参数：-d 从压缩包内删除指定文件-r 递归处理指定目录中的文件和子目录-m 将指定文件添加到指定压缩包-P 压缩包添加密码 实例：#压缩文件zip a.zip /etc/passwd#压缩目录zip -r a.zip /etc/passwd # r 递归#打包一类文件zip a.zip *.jpg unzip功能：解压ZIP压缩包 参数：-P 解压需要密码-d 文件解压到指定文件夹-o 不提示直接覆盖文件 实例：unzip a.zip -d /tmp rar/unrar功能：压缩/解压“.rar”文档。 实例：#将/etc 目录压缩为etc.rarrar a etc.rar /etc#将etc.rar 解压 rar x etc.rar unrar -e etc.tar]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tar</tag>
        <tag>zip</tag>
        <tag>rar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-文件权限管理]]></title>
    <url>%2F2018%2F04%2F09%2FLinux%2F2018-04-09-Linux-%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[文件管理权限 - rwx r-x r-x user1 user1 filename类型 拥有者权限 所属组权限 其他人权限 拥有者 属组 对象 文件权限内容解读 [-] [rwx][r-x][r-- ] 1 234 567 8901 为：代表这个文件名目录或 文件 ， (-)为文件；(d)为目录；234为：拥有者的权限，(r)可读、(w)可写、(x)可执行 文件 (r)可读、(w)创建、(x)进入 目录567为：同群组用户权利890为：其他用户权利 对于目录: r读（看到目录里面有什么） ls w建文件、删除、移动 touch mkdir rm mv cp x进入 cd 特殊权限 SUID SGID stickybitS对应值： 4s 2t 1 SUID限定：只能设置在二进制可行性程序上面，对目录文本设置无效。功能：程序运行的权限从执行者变更成程序的所有者。 [root@izuf6fmbh00axmyjllc0x0z ~]# ll /usr/bin/passwd-rwsr-xr-x. 1 root root 27832 Jun 10 2014 /usr/bin/passwd[root@izuf6fmbh00axmyjllc0x0z ~]# ll /usr/bin/less-rwxr-xr-x. 1 root root 158240 Jul 31 2015 /usr/bin/less[root@izuf6fmbh00axmyjllc0x0z ~]# chmod u+s /usr/bin/less[root@izuf6fmbh00axmyjllc0x0z ~]# ll /usr/bin/less-rwsr-xr-x. 1 root root 158240 Jul 31 2015 /usr/bin/less SGID限定：既可以给二进制可执行程序设置，也可以给目录设置功能：在设置了SGID权限的目录下建立文件时，新创建的文件的所属组会继承上级目录权限 Stickybit限定：只作用于目录功能：目录下创建的文件只有root、文件创建、目录所有者才能删除 chmod功能：用于改变文件或目录的访问权限。该命令有两种用法：一种是包含字母和操作符表达式的文字设定法，另一种是包含数字的数字设定法。 文字设定chmod [who] [+ | - | =] [mode] 文件名chmod [who] [数值] 文件名 命令中各选项的含义为：1. 操作对象who 可以是下述字母中的任一个或者它们的组合： u 表示用户(user) ，即文件或目录的所有者 g 表示同组(group)用户，即与文件属主有相同组ID 的所有用户 o 表示其他(others)用户 a 表示所有(all)用户，它是系统默认值。2. 操作符号可以是： + 添加某个权限 - 取消某个权限 = 赋予给定权限,并取消其他所有权限3. mode 表示权限常用的参数有 r 可读 w 可写 x 可执行 例如：1 、将文件 script 的权限设为可执行。命令如下： chmod =rx text 执行成功后 ，用ls -l script 命令查看文件属性的结果如下： -r-xr-xr-x 1 user group 0 Feb 10 09:42 script2 、将文件 text 的权限设为：文件属主可读、可写、可执行，与文件属主同组的用户可读，其他用户不可读。命令如下： chmod u=rwx,g=r,o= text （注意,后无空格o=后有空格） 执行成功后 ，用ls –l text 命令查看文件属性的结果如下： -rwxr—– 1 user group 0 Feb 10 09:42 text 数字设定每种身份(owner/group/others)各自的三个权限(r/w/x)分数是需要累加的。例如当权限为： [-rwxrwx—] 分数则是： owner = rwx = 4+2+1 = 7 group = rwx = 4+2+1 = 7 others= — = 0+0+0 = 0 chmod 777 text chown功能：用于更改某个文件或目录的属主和属组。例如root 用户把自己的一个文件拷贝给用户oracle ，为了让用户oracle 能够存取这个文件，root 用户应该把这个文件的属主设为oracle ，否则用户oracle无法存取这个文件。语法：chown [用户:组] 文件 umaskumask可用来设定[权限掩码]。[权限掩码]是由3个八进制的数字所组成。 [root@localhost ~]# umask0022 权限计算将现有的存取权限减掉权限掩码后，即可产生建立文件时预设的权限。（有些时候会报错）文件默认权限=666-umask目录默认权限=777-umask 正确的计算方式：6 6 6 umask 0 3 3 取反110 110 110 000 011 011 取反 111 100 100110 110 110 与 111 100 100 异或110 100 100 getfacl功能：取得某个文件/目录的ACL设置项目。[root@localhost ~]# getfacl test# file: test# owner: root# group: rootuser::rwxgroup::r-xother::r-x setfacl功能：设置某个文件/目录的ACL设置项目。参数：-m：设置后续acl参数 -x：删除后续acl参数 -b：删除全部的acl参数-k：删除默认的acl参数-R：递归设置acl，包括子目录-d：设置默认acl 具体实例：#创建一个文件[root@localhost ~]# touch test[root@localhost ~]# chmod 777 test[root@localhost ~]# getfacl test //获得文件的ACL权限# file: test //文件名# owner: root //文件所属者# group: root //文件所属组user::rwx //文件所属者权限group::rwx //同组用户权限other::rwx //其它者权限#修改其ACL策略，使用用户code只有读取的权限[root@localhost ~]# setfacl -m u:code:r test[root@localhost ~]# ll /test-rwxrwxrwx+ 1 root root 1 Apr 09 10:25 /test //可以看到权限的最后多了一个”+”号#再次查看一下此文件的ACL属性[root@Slocalhost ~]# getfacl test# file: test# owner: root# group: rootuser::rwxuser:code:r-- //可以看到code单独的权限为r--group::rwxmask::rwxother::rwx#设置有效权限为只写[root@localhost ~]# setfacl -m m:w test [root@localhost ~]# getfacl test# file: test# owner: root# group: rootuser::rwxuser:code:r-- group::rwx mask::-w- //可以看到有效权限已经修改成功other::rwx#测试权限[root@localhost ~]# echo "this is a test getfacl " &gt;/test[code@localhost ~]$ vim /test "/test" [Permission Denied] //设置权限后不能写入数据#取消之前加入的权限[root@localhost ~]# setfacl -x u:code test //取消/test对用户code的权限[root@localhost ~]# setfacl -x m test //恢复有效权限[root@localhost~]# getfacl test # file: test# owner: root# group: rootuser::rwxgroup::rwxother::rwx[root@localhost ~]# ll /test-rwxrwxrwx 1 root root 24 Apr 09 15:01 /test chattr功能：改变文件属性。8种文件属性a：让文件或目录仅供附加用途。b：不更新文件或目录的最后存取时间。c：将文件或目录压缩后存放。d：将文件或目录排除在倾倒操作之外。i：不得任意更动文件或目录。s：保密性删除文件或目录。S：即时更新文件或目录。u：预防意外删除。 语法：chattr [+|-|=&lt;属性&gt;][文件或目录…] chattr +a a.txt //只能追加chattr +i a.txt //不能编辑chattr -i a.txtchattr -a a.txt lsattr功能：显示文件特殊属性 [root@localhost ~]# lsattr a.txt-----a---------- a.txt]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>文件权限管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-用户管理(二)]]></title>
    <url>%2F2018%2F04%2F08%2FLinux%2F2018-04-08-Linux-%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[useradduseradd命令用于Linux中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 格式：useradd(选项)(参数)useradd [-d home] [-s shell] [-c comment] [-m [-k template]] [-f inactive] [-e expire ] [-p passwd] [-r] name选项：-c：加上备注文字，备注文字保存在passwd的备注栏中。-d：指定用户登入时的主目录，替换系统默认值/home/&lt;用户名&gt;-D：变更预设值。-e：指定账号的失效日期，日期格式为MM/DD/YY，例如06/30/12。缺省表示永久有效。-f：指定在密码过期后多少天即关闭该账号。如果为0账号立即被停用；如果为-1则账号一直可用。默认值为-1.-g：指定用户所属的群组。值可以使组名也可以是GID。用户组必须已经存在的，期默认值为100，即users。-G：指定用户所属的附加群组。-m：自动建立用户的登入目录。-M：不要自动建立用户的登入目录。-n：取消建立以用户名称为名的群组。-r：建立系统账号。-s：指定用户登入后所使用的shell。默认值为/bin/bash。-u：指定用户ID号。该值在系统中必须是唯一的。0~499默认是保留给系统用户账号使用的，所以该值必须大于499。 实例：[root@izuf6fmbh00axmyjllc0x0z ~]# useradd -u 5003 shen[root@izuf6fmbh00axmyjllc0x0z ~]# id shenuid=5003(shen) gid=5003(shen) groups=5003(shen) passwdpasswd命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 语法：passwd(选项)(参数)选项：-d：删除密码，仅有系统管理者才能使用；-f：强制执行；-k：设置只有在密码过期失效后，方能更新；-l：锁住密码；-s：列出密码的相关信息，仅有系统管理者才能使用；-u：解开已上锁的帐号。参数：用户名：需要设置密码的用户名。 #非交互式修改密码[root@izuf6fmbh00axmyjllc0x0z ~]# echo 123456 | passwd --stdin root #--stdin 非交互添加密码 userdel功能：删除用户。语法：userdel [-r][用户账号]参数：-r 删除用户目录及目录中所有内容 [root@loaclhost ~]# userdel -r rm[root@loaclhost ~]# ls /home/test rm1 rm2 groupadd用于创建一个新的工作组，新工作组的信息将被添加到系统文件中。语法：groupadd (选项) (参数)选项： -g：指定新建工作组的id； -r：创建系统工作组，系统工作组的组ID小于500； -K：覆盖配置文件“/ect/login.defs”； -o：允许添加组ID号不唯一的工作组。参数： 组名：指定新建工作组的组名。 实例： groupmod功能：修改群组名称。语法：groupmod [-g &lt;群组识别码&gt; ][-n &lt;新群组名称&gt;][群组名称]参数：-g &lt;群组识别码&gt; 设置欲使用的群组识别码。-o 重复使用群组识别码。-n &lt;新群组名称&gt; 设置欲使用的群组名称。 chgrpchgrp命令用来改变文件或目录所属的用户组。该命令用来改变指定文件所属的用户组。其中，组名可以是用户组的id，也可以是用户组的组名。文件名可以是由空格分开的要改变属组的文件列表，也可以是由通配符描述的文件集合。如果用户不是该文件的文件主或超级用户(root)，则不能改变该文件的组。 语法：chgrp(选项)(参数)选项：-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quiet或——silent：不显示错误信息；-h或--no-dereference：只对符号连接的文件作修改，而不是该其他任何相关文件；-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；-v或——verbose：显示指令执行过程；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同；参数： 组：指定新工作名称； 文件：指定要改变所属组的文件列表。多个文件或者目录之间使用空格隔开。 用户模版恢复解决模板文件被删之后显示不正常的问题 [root@localhost ~]# useradd top[root@localhost ~]# echo 123456 | passwd --stdin top [root@localhost ~]# ls -a /home/top/. .. .bash_logout .bash_profile .bashrc .mozilla .zshrc[root@localhost ~]# rm -rf !$.bash*rm -rf /home/top/.bash*[root@localhost ~]# su - top-bash-4.2$ #恢复[root@localhost ~]# cp /etc/skel/.bash* /home/top/[root@localhost ~]# chown top:top /home/top/.bash*]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>用户管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-用户管理(一)]]></title>
    <url>%2F2018%2F04%2F07%2FLinux%2F2018-04-07-Linux-%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[用户分类根据账号的位置: 本地账号、远程（域）账号、LDAP、NIS根据账号的功能: 超级用户 （root） UID : 0 普通用户 系统用户 UID：1-499 本地用户 UID：500 + 组分类根据账号的位置: 本地账号、远程（域）账号、LDAP、NIS根据账号的功能: 超级用户 （root） UID : 0 普通用户 系统用户 UID：1-999 本地用户 UID：1000 +每一个用户都有一个同名的组 关于系统用户和组的相关配置信息 账号信息 密码信息用户 /etc/passwd /etc/shadow组 /etc/group /etc/gshadow PASSWD格式解析/etc/passwd root:x:0:0:root:/root:/bin/bash格式解析说明：用户名：密码占位符：UID：GID：用户描述：用户目录（bash中“~”代表哪个）：登录后使用的shellpasswd 中伪用户 伪用户 含义 bin 拥有可执行的用户命令文件 sys 拥有系统文件 adm 拥有账户文件 uucp UUCP使用 ip Ip或Ipd子系统使用 nobody NFS使用 SHADOW文件格式解析/etc/shadowroot:$6$XjH3wXGQ$JWFg7kDEBixARtCVdTENdeGJS67LMslCENKcgTsGgajrqqGQUBtKGVBM6QCz.HJrNCbnU7JtH0Qd53V3U0hp60:17557:0:99999:7:::1、用户名: 用户的名称，对应/etc/passwd文件中2、密码: Centos6中使用MD5加密算法，Centos7中使用sha1的算法，第一个$后面加密算法类型，第二个$后面表示salt,第三个$后面表示密码的提取码（参照加密类型）3、上一次修改密码的时间: 指用户上次修改密码的时间，计算方法:从Linux元年1970年01月01日0点0分到目录所经过的秒数4、密码最小使用期限: 指用户修改密码后，需要到多少天后方可更改密码,0表示禁用5、密码过期时间: 指用户的密码到多少天后需要修改密码6、警告时间: 指用户密码到期前多少天提示用户修改密码，0和空字段表示禁用此功能7、宽限天数：密码过期了几天内还能改密码8、帐户过期时间: 批帐户在密码过期后多少天还未修改密码，将被停用9、保留 GROUP文件解析/etc/grouproot:x:0:组名：组密码占位符：GID：组成员（用户名）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>用户管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-查看文件]]></title>
    <url>%2F2018%2F04%2F02%2FLinux%2F2018-04-02-Linux%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Linux 查看文件 cat 由第一行开始显示文件内容 tac 从最后一行开始显示 nl 显示的时候输出行号 more 一页一页的显示文件内容 less 与 more 类似，但是比 more 更好的是，他可以往前翻页 head 只看头几行 tail 只看最后几行 cat作用：从第一行开始查看文件内容语法：cat 文件名cat /etc/passwd tac作用：从最后一行开始显示文件内容，与cat相反语法：tac 文件名cat /etc/passwd nl作用：显示行号语法：nl 文件名nl /etc/passed more作用：分页显示文件内容，不能向上翻页语法：more 文件名more /etc/passwd快捷键： 空白键 (space)：代表向下翻一页； Enter ：代表向下翻『一行』； /字串 ：向下搜寻『字串』这个关键字； :f ：立刻显示出档名以及目前显示的行数； q ：退出显示。 b 或 [ctrl]-b ：代表往回翻页，不过这动作只对文件有用。 less作用：分页显示文件内容，可以上下翻页语法：less 文件名less /etc/passwd快捷键： 空白键 ：向下翻动一页 [pagedown]：向下翻动一页 [pageup] ：向上翻动一页 /字串 ：向下搜寻『字串』 ?字串 ：向上搜寻『字串』 n ：重复前一个搜寻 N ：反向的重复前一个搜寻 q ：离开 less 这个程序 head作用：显示文件前几行语法：head -n 文件名head -5 /etc/passwd tail作用：显示文件最后几行语法：tail -n 文件名tail -5 /etc/passwd]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>查看文件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-文件与目录管理]]></title>
    <url>%2F2018%2F03%2F30%2FLinux%2F2018-03-30-Linux%E7%9B%AE%E5%BD%95%E5%A4%84%E7%90%86%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux文件与目录管理Linux目录为树状结构，最顶级目录为根目录/ 。 绝对路径：由根目录/开始，例如 /etc/bin 相对路径：不是由/开始，例如由 /usr/temp/doc 要到 /usr/temp/ppt 底下时，可以写成： cd ../ppt 处理目录的常用命令 ls: 列出目录 cd：切换目录 pwd：显示目前的目录 mkdir：创建一个新的目录 rmdir：删除一个空的目录 cp: 复制文件或目录 rm: 移除文件或目录 ls 列出目录语法： ls [参数] 选项与参数：-a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用)-d ：仅列出目录本身，而不是列出目录内的文件数据(常用)-l ：长数据串列出，包含文件的属性与权限等等数据；(常用)[root@zhangq ~]# lsanaconda-ks.cfg cd 切换目录cd是Change Directory的缩写，这是用来变换工作目录的命令。 语法： cd [相对路径或绝对路径] #使用 mkdir 命令创建 test 目录[root@zhangq ~]# mkdir test#使用绝对路径切换到 test 目录[root@zhangq ~]# cd /root/test/#使用相对路径切换到 test 目录[root@zhangq ~]# cd ./test/#表示回到自己的家目录，亦即是 /root 这个目录[root@zhangq test]# cd ~#表示去到目前的上一级目录，亦即是 /root 的上一级目录的意思；[root@zhangq ~]# cd .. pwd 显示目前所在的目录pwd 是 Print Working Directory 的缩写，也就是显示目前所在目录的命令。 选项与参数：-P：显示出真实的路径，而非使用链接（link）路径。实例：[root@cinder ~]# pwd /root[root@cinder ~]# pwd -P/root[root@zhangq ~]# cd /var/mail &lt;==注意，/var/mail是一个连结档[root@zhangq mail]# pwd/var/mail &lt;==列出目前的工作目录[root@zhangq mail]# pwd -P/var/spool/mail &lt;==怎么回事？有没有加 -P 差很多～[root@zhangq mail]# ls -ld /var/maillrwxrwxrwx 1 root root 10 Sep 4 17:54 /var/mail -&gt; spool/mail#看到这里应该知道为啥了吧？因为 /var/mail 是连结档，连结到 /var/spool/mail #所以，加上 pwd -P 的选项后，会不以连结档的数据显示，而是显示正确的完整路径啊！ mkdir 创建文件夹语法：mkdir [-mp] 目录名称 选项与参数：-m ：配置文件的权限！直接配置-p ：递归创建目录(包含上一级目录)实例：创建目录[root@zhangq ~]# cd /tmp[root@zhangq tmp]# mkdir test &lt;==创建一名为 test 的新目录[root@zhangq tmp]# mkdir test1/test2/mkdir: cannot create directory `test1/test2/&apos;: No such file or directory &lt;== 没办法直接创建此目录啊！[root@zhangq tmp]# mkdir -p test1/test2实例：创建权限为rwx--x--x 的目录。[root@zhangq tmp]# mkdir -m 711 test2[root@zhangq tmp]# ls -ldrwxr-xr-x 3 root root 4096 Jul 18 12:50 testdrwxr-xr-x 3 root root 4096 Jul 18 12:53 test1drwx--x--x 2 root root 4096 Jul 18 12:54 test2 rmdir 删除文件夹语法： rmdir 目录名称实例：[root@zhangq tmp]# rmdir test不常用，习惯使用rm替代rmdir cp 拷贝语法:cp [options] [原文件或目录][目标目录] 选项与参数：-a：相当于 -pdr 的意思， - pdr 请参考下列说明；(常用)-d：若来源档为连结档的属性(link file)，则复制连结档属性而非文件本身；-f：为强制(force)的意思，若目标文件已经存在且无法开启，则移除后再尝试一次；-i：若目标档(destination)已经存在时，在覆盖时会先询问动作的进行(常用)-l：进行硬式连结(hard link)的连结档创建，而非复制文件本身；-p：连同文件的属性一起复制过去，而非使用默认属性(备份常用)；-r：递归持续复制，用於目录的复制行为；(常用)-s：复制成为符号连结档 (symbolic link)，亦即『捷径』文件；-u：若 destination 比 source 旧才升级 destination ！实例：#将目录/root/tmp1/test 复制到目录/root/work下[root@zhangq tmp]cp -r /root/tmp1/test /root/work#将/root/tmp1/test1目录下的test1和test2文件复制到root/work下，并保持目录属性 [root@zhangq tmp]cp -rp /root/tmp1/test1 /root/tmp1/test2 /root/work rm 删除文件或目录语法：rm [-fir][文件或目录] 选项与参数：-f：强制执行-i：互动模式-r：递归删除实例：[root@zhangq tmp]rm -rf /root/test mv 剪切或重命名语法：mv [-ifu][原文件或目录][目标目录] 选项与参数：-f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖；-i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！-u ：若目标文件已经存在，且 source 比较新，才会升级 (update)实例：#复制一文件，创建一目录，将文件移动到目录中[root@zhangq ~]# cd /tmp[root@zhangq tmp]# cp ~/.bashrc bashrc#将某个文件移动到某个目录[root@zhangq tmp]# mkdir mvtest[root@zhangq tmp]# mv bashrc mvtest#重命名[root@zhangq tmp]# mv mvtest mvtest2]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>文件与目录管理</tag>
      </tags>
  </entry>
</search>
